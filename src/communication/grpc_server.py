"""
gRPC server implementation for distributed inference.
"""

import grpc
from concurrent import futures
import time
import logging
from typing import Optional, Dict, Any, List
import mlx.core as mx
import psutil

from ..core.config import ClusterConfig, DeviceConfig
from .tensor_utils import serialize_mlx_array, deserialize_mlx_array

# Import will be generated by protoc
# from . import inference_pb2, inference_pb2_grpc

logger = logging.getLogger(__name__)


class InferenceServicer:
    """gRPC service implementation for inference."""
    
    def __init__(self, config: ClusterConfig, device_config: DeviceConfig, 
                 layer_processor: Optional[Any] = None):
        self.config = config
        self.device_config = device_config
        self.layer_processor = layer_processor
        self.start_time = time.time()
        self.request_count = 0
        
    def ProcessLayers(self, request, context):
        """Process assigned layers for the input tensor."""
        try:
            start_time = time.time()
            self.request_count += 1
            
            # Deserialize input tensor
            metadata = {
                'shape': list(request.metadata.shape),
                'dtype': request.metadata.dtype,
                'compressed': request.metadata.compressed
            }
            input_tensor = deserialize_mlx_array(request.input_tensor, metadata)
            
            logger.info(f"Processing layers {request.layer_indices} for request {request.request_id}")
            
            # Process layers if processor is available
            if self.layer_processor:
                output_tensor = self.layer_processor.process(
                    input_tensor, 
                    request.layer_indices,
                    context=dict(request.context)
                )
            else:
                # Mock processing for testing
                logger.warning("No layer processor available, returning input as output")
                output_tensor = input_tensor
            
            # Serialize output
            output_data, output_metadata = serialize_mlx_array(
                output_tensor, 
                compress=request.metadata.compressed
            )
            
            # Build response
            from . import inference_pb2
            response = inference_pb2.LayerResponse(
                request_id=request.request_id,
                output_tensor=output_data,
                metadata=inference_pb2.TensorMetadata(
                    shape=output_metadata['shape'],
                    dtype=output_metadata['dtype'],
                    compressed=output_metadata['compressed']
                ),
                processing_time_ms=(time.time() - start_time) * 1000,
                device_id=self.device_config.device_id
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Error processing layers: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            raise
    
    def HealthCheck(self, request, context):
        """Return health status of this device."""
        from . import inference_pb2
        
        # Get GPU utilization
        gpu_util = self._get_gpu_utilization()
        
        # Get memory usage
        memory = psutil.virtual_memory()
        
        return inference_pb2.HealthStatus(
            healthy=True,
            device_id=self.device_config.device_id,
            timestamp=int(time.time()),
            details={
                'uptime_seconds': str(int(time.time() - self.start_time)),
                'request_count': str(self.request_count),
                'gpu_utilization': f"{gpu_util:.1f}%",
                'memory_usage': f"{memory.percent:.1f}%"
            }
        )
    
    def GetDeviceInfo(self, request, context):
        """Return device information."""
        from . import inference_pb2
        
        # Get assigned layers
        assigned_layers = self.config.model.get_device_layers(self.device_config.device_id)
        
        # Get memory usage
        memory = psutil.virtual_memory()
        memory_used_gb = (memory.total - memory.available) / (1024**3)
        
        # Convert capabilities to string map
        capabilities = {}
        if self.device_config.capabilities:
            caps = self.device_config.capabilities
            capabilities = {
                'model': caps.model,
                'memory_gb': str(caps.memory_gb),
                'gpu_cores': str(caps.gpu_cores),
                'cpu_cores': str(caps.cpu_cores),
                'neural_engine_cores': str(caps.neural_engine_cores),
                'bandwidth_gbps': str(caps.bandwidth_gbps)
            }
        
        return inference_pb2.DeviceInfo(
            device_id=self.device_config.device_id,
            hostname=self.device_config.hostname,
            rank=self.device_config.rank,
            role=self.device_config.role.value,
            assigned_layers=assigned_layers,
            capabilities=capabilities,
            gpu_utilization=self._get_gpu_utilization(),
            memory_usage_gb=memory_used_gb
        )
    
    def _get_gpu_utilization(self) -> float:
        """Get GPU utilization percentage."""
        # This is a placeholder - actual implementation would use
        # platform-specific tools to get real GPU usage
        # For macOS, we'd parse output from powermetrics
        return 0.0


def create_grpc_server(config: ClusterConfig, device_config: DeviceConfig,
                      layer_processor: Optional[Any] = None,
                      max_workers: int = 10) -> grpc.Server:
    """
    Create and configure a gRPC server.
    
    Args:
        config: Cluster configuration
        device_config: This device's configuration
        layer_processor: Optional layer processor implementation
        max_workers: Maximum number of worker threads
        
    Returns:
        Configured gRPC server
    """
    # Import generated code
    from . import inference_pb2_grpc
    
    # Create server with increased message size limits
    options = [
        ('grpc.max_send_message_length', 512 * 1024 * 1024),  # 512MB
        ('grpc.max_receive_message_length', 512 * 1024 * 1024),  # 512MB
    ]
    
    server = grpc.server(
        futures.ThreadPoolExecutor(max_workers=max_workers),
        options=options
    )
    
    # Add servicer
    servicer = InferenceServicer(config, device_config, layer_processor)
    inference_pb2_grpc.add_InferenceServiceServicer_to_server(servicer, server)
    
    # Bind to port
    address = f'[::]:{device_config.grpc_port}'
    server.add_insecure_port(address)
    
    logger.info(f"gRPC server created for {device_config.device_id} on {address}")
    
    return server


async def start_grpc_server(config: ClusterConfig, device_config: DeviceConfig,
                           layer_processor: Optional[Any] = None) -> None:
    """
    Start the gRPC server and keep it running.
    
    Args:
        config: Cluster configuration
        device_config: This device's configuration
        layer_processor: Optional layer processor implementation
    """
    server = create_grpc_server(config, device_config, layer_processor)
    
    server.start()
    logger.info(f"gRPC server started on port {device_config.grpc_port}")
    
    try:
        # Keep server running
        while True:
            await asyncio.sleep(3600)  # Sleep for an hour
    except KeyboardInterrupt:
        logger.info("Shutting down gRPC server...")
        server.stop(grace_period=10)
        
import asyncio