2025-08-06 23:46:36,170 - INFO - Loading model: mlx-community/Qwen3-1.7B-8bit
2025-08-06 23:46:36,160 - INFO - Loading model: mlx-community/Qwen3-1.7B-8bit
2025-08-06 23:46:36,213 - INFO - Distributed initialized: rank=0, world_size=2
2025-08-06 23:46:36,213 - INFO - Rank 0: Downloading model metadata...
2025-08-06 23:46:36,235 - INFO - Distributed initialized: rank=1, world_size=2
2025-08-06 23:46:36,235 - INFO - Rank 1: Downloading model metadata...
2025-08-06 23:46:36,214 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-08-06 23:46:36,237 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-08-06 23:46:36,391 - DEBUG - https://huggingface.co:443 "GET /api/models/mlx-community/Qwen3-1.7B-8bit/revision/main HTTP/1.1" 200 6006
2025-08-06 23:46:36,415 - DEBUG - https://huggingface.co:443 "GET /api/models/mlx-community/Qwen3-1.7B-8bit/revision/main HTTP/1.1" 200 6006
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 118149.41it/s]
2025-08-06 23:46:36,406 - INFO - Rank 0: Lazy loading model to determine sharding...
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 109655.01it/s]
2025-08-06 23:46:36,434 - INFO - Rank 1: Lazy loading model to determine sharding...
2025-08-06 23:46:36,423 - WARNING - Model mlx-community/Qwen3-1.7B-8bit doesn't have native pipeline() support
2025-08-06 23:46:36,423 - INFO - Adding custom pipeline() implementation...
2025-08-06 23:46:36,423 - INFO - Added pipeline() method to Qwen3Model
2025-08-06 23:46:36,423 - INFO - âœ… Custom pipeline() method added successfully!
2025-08-06 23:46:36,423 - INFO - Rank 0: Applying pipeline sharding...
2025-08-06 23:46:36,423 - INFO - Applying pipeline parallelism: rank 0/2
2025-08-06 23:46:36,423 - INFO - Found 28 transformer layers
2025-08-06 23:46:36,423 - INFO - Rank 0: Assigned layers 0-13
2025-08-06 23:46:36,423 - INFO - Rank 0: Owns layers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
2025-08-06 23:46:36,423 - INFO - Rank 0: has_embeddings=True, has_output=False
2025-08-06 23:46:36,423 - INFO - Rank 0: Pipeline setup complete
2025-08-06 23:46:36,424 - INFO - Rank 0: Downloading 1 weight files for this shard...
2025-08-06 23:46:36,455 - WARNING - Model mlx-community/Qwen3-1.7B-8bit doesn't have native pipeline() support
2025-08-06 23:46:36,455 - INFO - Adding custom pipeline() implementation...
2025-08-06 23:46:36,455 - INFO - Added pipeline() method to Qwen3Model
2025-08-06 23:46:36,455 - INFO - âœ… Custom pipeline() method added successfully!
2025-08-06 23:46:36,455 - INFO - Rank 1: Applying pipeline sharding...
2025-08-06 23:46:36,455 - INFO - Applying pipeline parallelism: rank 1/2
2025-08-06 23:46:36,455 - INFO - Found 28 transformer layers
2025-08-06 23:46:36,455 - INFO - Rank 1: Assigned layers 14-27
2025-08-06 23:46:36,455 - INFO - Rank 1: Owns layers [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
2025-08-06 23:46:36,455 - INFO - Rank 1: has_embeddings=False, has_output=True
2025-08-06 23:46:36,455 - INFO - Rank 1: Pipeline setup complete
2025-08-06 23:46:36,457 - INFO - Rank 1: Downloading 1 weight files for this shard...
2025-08-06 23:46:36,546 - DEBUG - https://huggingface.co:443 "GET /api/models/mlx-community/Qwen3-1.7B-8bit/revision/main HTTP/1.1" 200 6006
Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10727.12it/s]
2025-08-06 23:46:36,581 - DEBUG - https://huggingface.co:443 "GET /api/models/mlx-community/Qwen3-1.7B-8bit/revision/main HTTP/1.1" 200 6006
Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2824.45it/s]
2025-08-06 23:46:36,827 - INFO - Rank 0: Loading model weights for this shard...
2025-08-06 23:46:36,883 - INFO - Rank 1: Loading model weights for this shard...
2025-08-06 23:46:36,915 - INFO - Rank 0: GPU memory after loading = 1.70 GB
2025-08-06 23:46:36,962 - INFO - Rank 1: GPU memory after loading = 1.70 GB
2025-08-06 23:46:37,009 - INFO - Rank 0: Ready for distributed inference!
2025-08-06 23:46:37,010 - INFO - Starting API server on rank 0
2025-08-06 23:46:37,034 - INFO - Rank 1: Ready for distributed inference!
2025-08-06 23:46:37,034 - INFO - Worker rank 1 ready for distributed processing
[32mINFO[0m:     Started server process [[36m8423[0m]
[32mINFO[0m:     Waiting for application startup.
[32mINFO[0m:     Application startup complete.
[32mINFO[0m:     Uvicorn running on [1mhttp://0.0.0.0:8100[0m (Press CTRL+C to quit)
2025-08-06 23:46:56,871 - DEBUG - Token 1: '<think>'
2025-08-06 23:46:56,892 - DEBUG - Token 2: '
'
2025-08-06 23:46:56,913 - DEBUG - Token 3: 'Okay'
2025-08-06 23:46:56,932 - DEBUG - Token 4: ','
2025-08-06 23:46:56,952 - DEBUG - Token 5: ' the'
2025-08-06 23:46:56,967 - INFO - âœ… Generated 5 tokens using 2 GPUs
2025-08-06 23:46:56,967 - INFO - Prompt: 10 tokens, 116.0 tok/s
2025-08-06 23:46:56,967 - INFO - Generation: 5 tokens, 61.7 tok/s
2025-08-06 23:46:56,967 - INFO - Peak memory: 1.70 GB
2025-08-06 23:46:56,967 - INFO - Generated text: '' (length: 0)
[32mINFO[0m:     127.0.0.1:58048 - "[1mPOST /v1/chat/completions HTTP/1.1[0m" [32m200 OK[0m
[32mINFO[0m:     Shutting down
[32mINFO[0m:     Waiting for application shutdown.
[32mINFO[0m:     Application shutdown complete.
[32mINFO[0m:     Finished server process [[36m8423[0m]
/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py:301: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown: {'/mp-k61pjat0'}
  warnings.warn(
