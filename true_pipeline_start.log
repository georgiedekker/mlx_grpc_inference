[0;34m=== ğŸš€ Auto-Discovery MLX Distributed Inference ===[0m
[0;34mğŸ”§ Optimizing system for MLX distributed inference...[0m
[0;32mâœ“ GPU memory limit already optimized: 100000MB[0m
[0;32mâœ… System optimized for MLX[0m
[0;34mğŸ” Discovering Apple Silicon devices...[0m
Local machine Thunderbolt IP: 192.168.5.1
Testing 192.168.5.1 (local)...
  âœ“ Local machine
  ğŸ Apple Silicon detected: Apple M4
  ğŸ’¾ Memory: 16 GB
  ğŸ·ï¸  Hostname: mini1.local
Scanning Thunderbolt network for remote device...
Checking Thunderbolt devices: 192.168.5.1 192.168.5.2
Testing 192.168.5.2 (remote)...
  âœ“ SSH accessible
  ğŸ Apple Silicon detected: Apple M4
  ğŸ’¾ Memory: 16 GB
  ğŸ·ï¸  Hostname: mini2.local

[0;32mğŸ¯ Discovery Complete![0m
Found 2 Apple Silicon devices:
  â€¢ 192.168.5.1
  â€¢ 192.168.5.2
Total cluster memory: 32GB

[0;32mğŸš€ Using DeepSeek-R1 with NATIVE pipeline() support[0m
[0;34mThis model supports TRUE pipeline parallelism[0m
[0;34mTotal cluster memory: 32GB[0m
[0;31mâŒ Model not found: mlx-community/DeepSeek-R1-3bit[0m
[0;33mThis model is REQUIRED for pipeline parallelism.[0m
[0;33mPlease download it first with:[0m
[0;34mcd ~/Movies/mlx_grpc_inference && source .venv/bin/activate[0m
[0;34mpython -c "from mlx_lm import load; load('mlx-community/DeepSeek-R1-3bit')"[0m

[0;33mNote: This is a ~15GB download and may take a while.[0m
