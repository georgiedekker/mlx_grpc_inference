#!/usr/bin/env python3
"""
Hybrid distributed server - gRPC orchestration with MLX optimized tensor ops.
Uses Thunderbolt network for communication between devices.
"""
import os
import asyncio
import grpc
import logging
import time
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
import mlx.core as mx
from mlx_lm import load, generate
from mlx_lm.models.base import create_attention_mask, create_causal_mask
from mlx_lm.models.cache import KVCache
from typing import List, Dict, Any
from concurrent import futures

from src.communication import inference_pb2, inference_pb2_grpc
from src.communication.tensor_utils import serialize_mlx_array, deserialize_mlx_array

# Wrapper functions to match proto format
def serialize_tensor(array: mx.array) -> inference_pb2.Tensor:
    """Serialize MLX array to proto Tensor."""
    data, metadata = serialize_mlx_array(array, compress=True, compression_algorithm="lz4")
    tensor = inference_pb2.Tensor()
    tensor.data = data
    tensor.shape.extend(metadata['shape'])
    tensor.dtype = metadata['dtype']
    return tensor

def deserialize_tensor(tensor: inference_pb2.Tensor) -> mx.array:
    """Deserialize proto Tensor to MLX array."""
    metadata = {
        'shape': list(tensor.shape),
        'dtype': tensor.dtype,
        'compressed': True,
        'compression_info': {'algorithm': 'lz4'}
    }
    return deserialize_mlx_array(tensor.data, metadata)

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# CRITICAL: Set GPU as default device for all operations
mx.set_default_device(mx.gpu)
logger.info("Default device set to GPU")

# FastAPI app
app = FastAPI(title="MLX Distributed Inference")

# Request/Response models
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    max_tokens: int = 100
    temperature: float = 0.7

class ChatResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, Any]  # Changed to Any to support float values

# Global state
model = None
tokenizer = None
worker_stubs = []
model_config = None

# Mask cache to avoid recreating masks
mask_cache = {}
MAX_SEQUENCE_LENGTH = 2048  # Maximum sequence length we'll support

# KV cache storage for local layers
local_kv_caches = {}  # session_id -> {layer_idx -> cache}
local_cache_timestamps = {}  # session_id -> last_access_time

# Configuration - default value, can be overridden per request
DEFAULT_USE_DISTRIBUTED_INFERENCE = 'auto'  # 'always', 'never', 'auto'

def initialize_coordinator():
    """Initialize the coordinator with model metadata and worker connections."""
    global model, tokenizer, worker_stubs, model_config
    
    # CRITICAL: Clear any existing stubs
    worker_stubs = []
    
    # Load model to get configuration
    model_name = "mlx-community/Qwen3-1.7B-8bit"
    logger.info(f"Loading model configuration from {model_name}")
    model, tokenizer = load(model_name)
    
    # CRITICAL: Test the model locally first to ensure it works
    logger.info("Testing model with single-device inference...")
    # Use proper chat template for testing, just like actual inference
    test_messages = [{"role": "user", "content": "Hello"}]
    test_prompt = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)
    test_tokens = tokenizer.encode(test_prompt)
    
    with mx.stream(mx.gpu):
        # Test embeddings
        test_input = mx.array([test_tokens])
        test_embed = model.model.embed_tokens(test_input)
        mx.eval(test_embed)
        logger.info(f"Test embeddings - mean: {test_embed.mean():.6f}, std: {test_embed.std():.6f}")
        
        # Test full forward pass
        hidden = test_embed
        for i, layer in enumerate(model.model.layers):
            hidden = layer(hidden)
            if i % 5 == 0:  # Log every 5 layers
                mx.eval(hidden)
                # Don't log every 5 layers - it's too verbose and std can be high
                if i == len(model.model.layers) - 1:  # Only log the last layer
                    logger.info(f"Test final layer {i} - mean: {hidden.mean():.6f}, std: {hidden.std():.6f}")
        
        # Test final projection
        hidden = model.model.norm(hidden)
        if hasattr(model.model, 'embed_tokens') and hasattr(model.model.embed_tokens, 'as_linear'):
            logits = model.model.embed_tokens.as_linear(hidden)
        mx.eval(logits)
        logger.info(f"Test logits - shape: {logits.shape}, mean: {logits.mean():.6f}, std: {logits.std():.6f}")
        
        # Log final stats for debugging
        final_std = float(hidden.std())
        if final_std > 50.0:
            logger.info(f"Note: std deviation {final_std:.2f} is normal for Qwen3-1.7B-8bit model")
    
    # Get model configuration
    model_config = {
        "n_layers": len(model.model.layers),
        "vocab_size": model.model.vocab_size,
        "model_type": model.model_type,
    }
    logger.info(f"Model has {model_config['n_layers']} layers")
    
    # Connect to workers on Thunderbolt network - ONLY 2 DEVICES
    worker_addresses = [
        "192.168.5.1:50051",  # mini1 (coordinator also acts as worker)
        "192.168.5.2:50051",  # mini2 
    ]
    
    # Distribute layers across workers
    layers_per_worker = model_config['n_layers'] // len(worker_addresses)
    remainder = model_config['n_layers'] % len(worker_addresses)
    
    layer_assignments = []
    start_layer = 0
    
    logger.info(f"Distributing {model_config['n_layers']} layers across {len(worker_addresses)} workers")
    logger.info(f"Base layers per worker: {layers_per_worker}, remainder: {remainder}")
    
    for i, addr in enumerate(worker_addresses):
        # Give extra layers to first workers if there's a remainder
        num_layers = layers_per_worker + (1 if i < remainder else 0)
        end_layer = start_layer + num_layers
        
        layer_assignments.append({
            "address": addr,
            "start_layer": start_layer,
            "end_layer": end_layer,
            "is_first": i == 0,
            "is_last": i == len(worker_addresses) - 1
        })
        
        logger.info(f"Worker {i} ({addr}): layers {start_layer}-{end_layer-1} ({num_layers} layers)")
        start_layer = end_layer
    
    # Connect to workers
    for assignment in layer_assignments:
        addr = assignment["address"]
        logger.info(f"Connecting to worker at {addr} (layers {assignment['start_layer']}-{assignment['end_layer']-1})")
        
        if addr.startswith("192.168.5.1"):
            # Local worker - we'll handle this directly
            worker_stubs.append({
                "stub": None,  # Local processing
                "assignment": assignment
            })
        else:
            # Remote worker
            channel = grpc.insecure_channel(addr, options=[
                ('grpc.max_send_message_length', 500 * 1024 * 1024),
                ('grpc.max_receive_message_length', 500 * 1024 * 1024),
            ])
            stub = inference_pb2_grpc.InferenceServiceStub(channel)
            worker_stubs.append({
                "stub": stub,
                "assignment": assignment
            })
    
    logger.info(f"Coordinator initialized successfully with {len(worker_stubs)} workers")

async def distributed_forward(input_ids: mx.array, session_id: str = None, is_prompt: bool = True) -> mx.array:
    """Forward pass through distributed model with KV cache support."""
    current_hidden = None
    total_start = time.time()
    
    if session_id is None:
        import uuid
        session_id = str(uuid.uuid4())
    
    # Monitor memory at start
    mem_info = mx.metal.device_info() or {}
    start_memory = mem_info.get('used_memory', 0) / 1e9
    
    tokens_count = input_ids.shape[1]
    pass_type = "prompt" if is_prompt else "incremental"
    logger.info(f"{pass_type.capitalize()} forward pass with {tokens_count} token(s) - Memory: {start_memory:.2f}GB, session={session_id}")
    
    # DEBUG: Compare with single-device forward pass
    logger.info(f"DEBUG: Input shape: {input_ids.shape}, tokens: {input_ids.tolist()[0][:10] if input_ids.shape[1] > 10 else input_ids.tolist()[0]}...")
    
    # Get embeddings (on first worker)
    if worker_stubs[0]["stub"] is None:
        # Local processing
        with mx.stream(mx.gpu):
            current_hidden = model.model.embed_tokens(input_ids)
            mx.eval(current_hidden)
        logger.info(f"DEBUG: After embeddings - mean: {current_hidden.mean():.6f}, std: {current_hidden.std():.6f}")
    else:
        # Remote processing
        request = inference_pb2.ForwardRequest()
        request.input_ids.extend(input_ids.tolist()[0])
        request.is_embedding = True
        
        response = worker_stubs[0]["stub"].Forward(request)
        current_hidden = deserialize_tensor(response.output)
        logger.info(f"DEBUG: After embeddings (remote) - mean: {current_hidden.mean():.6f}, std: {current_hidden.std():.6f}")
    
    # Handle attention mask based on pass type
    T = current_hidden.shape[1]
    if is_prompt and T > 1:
        # For prompt processing, use full causal mask
        if T not in mask_cache:
            mask_cache[T] = create_causal_mask(T, offset=0)
            logger.info(f"Created and cached causal mask for T={T}")
        attention_mask = mask_cache[T]
    else:
        # For incremental generation, no mask needed (single token)
        attention_mask = None
    
    # Process through transformer layers
    for worker_info in worker_stubs:
        assignment = worker_info["assignment"]
        stub = worker_info["stub"]
        
        logger.info(f"DEBUG: Before layers {assignment['start_layer']}-{assignment['end_layer']-1} - mean: {current_hidden.mean():.6f}, std: {current_hidden.std():.6f}")
        
        if stub is None:
            # Local processing with KV cache support
            layer_start = time.time()
            num_layers = assignment["end_layer"] - assignment["start_layer"]
            logger.info(f"Processing {num_layers} layers ({assignment['start_layer']}-{assignment['end_layer']-1}) locally, session={session_id}, is_prompt={is_prompt}")
            
            # Get or create KV cache for this session
            if session_id not in local_kv_caches:
                local_kv_caches[session_id] = {}
                logger.info(f"Created new local KV cache for session {session_id}")
            
            session_cache = local_kv_caches[session_id]
            
            with mx.stream(mx.gpu):
                # Use stored cache for incremental, fresh for prompt
                for idx, i in enumerate(range(assignment["start_layer"], assignment["end_layer"])):
                    layer_key = f"layer_{i}"
                    
                    if is_prompt or layer_key not in session_cache:
                        # First pass or no cache - create new KVCache object
                        layer_cache = KVCache()
                        session_cache[layer_key] = layer_cache
                        if is_prompt:
                            logger.debug(f"Creating fresh KVCache for layer {i} (prompt)")
                    else:
                        # Use existing cache for incremental generation
                        layer_cache = session_cache[layer_key]
                        logger.debug(f"Reusing KVCache for layer {i} (incremental)")
                    
                    # Process layer with cache (cache is updated in-place)
                    current_hidden = model.model.layers[i](current_hidden, attention_mask, layer_cache)
                    
                mx.eval(current_hidden)
                mx.synchronize()  # CRITICAL: Ensure all operations complete
            
            layer_time = time.time() - layer_start
            layers_per_second = num_layers / layer_time if layer_time > 0 else 0
            logger.info(f"Local layers took {layer_time:.3f}s ({layers_per_second:.1f} layers/s)")
        else:
            # Remote processing
            layer_start = time.time()
            logger.info(f"Sending to worker at {assignment['address']} for layers {assignment['start_layer']}-{assignment['end_layer']-1}")
            
            # DEBUG: Log what we're sending
            logger.info(f"DEBUG: Sending tensor with shape {current_hidden.shape}, mean: {current_hidden.mean():.6f}, std: {current_hidden.std():.6f}")
            
            # CRITICAL: Synchronize before serialization to ensure tensor is ready
            mx.synchronize()
            
            request = inference_pb2.LayerRequestV2()
            request.input_tensor.CopyFrom(serialize_tensor(current_hidden))
            request.start_layer = assignment["start_layer"]
            request.end_layer = assignment["end_layer"]
            request.session_id = session_id
            request.is_prompt = is_prompt
            # Don't send mask - worker will create it locally from sequence length
            
            send_time = time.time()
            response = stub.ProcessLayers(request)
            recv_time = time.time()
            current_hidden = deserialize_tensor(response.output_tensor)
            logger.info(f"Remote layers took {recv_time - layer_start:.3f}s (send: {send_time - layer_start:.3f}s, process+recv: {recv_time - send_time:.3f}s)")
        
        logger.info(f"DEBUG: After layers {assignment['start_layer']}-{assignment['end_layer']-1} - mean: {current_hidden.mean():.6f}, std: {current_hidden.std():.6f}")
        
        # Check for numerical instability - high std is normal for Qwen3-1.7B-8bit
        hidden_std = float(current_hidden.std())
        if hidden_std > 200.0:  # Increased threshold for this quantized model
            logger.error(f"CRITICAL: Hidden state std deviation {hidden_std:.2f} is too high! Model computation corrupted!")
            logger.error(f"Worker at {assignment['address']} (layers {assignment['start_layer']}-{assignment['end_layer']-1}) is producing corrupted outputs!")
            logger.error(f"This typically indicates:")
            logger.error(f"  1. Model weights are corrupted on the worker")
            logger.error(f"  2. Different model versions between devices")
            logger.error(f"  3. Tensor serialization/deserialization error")
            
            # Force single-device mode as fallback
            logger.error("FORCING SINGLE-DEVICE MODE DUE TO CORRUPTION")
            from mlx_lm.sample_utils import make_sampler
            sampler = make_sampler(temp=0.7)
            result = generate(model, tokenizer, prompt="Hello", max_tokens=50, sampler=sampler)
            logger.info(f"Single-device test result: {result}")
            
            raise ValueError(f"Numerical instability detected: std={hidden_std:.2f} from worker {assignment['address']}")
        elif hidden_std > 50.0:
            # Log high but acceptable std
            logger.info(f"Note: Hidden state std {hidden_std:.2f} is high but normal for Qwen3-1.7B-8bit")
    
    # Final projection (on last worker)
    logger.info(f"DEBUG: Before final projection - mean: {current_hidden.mean():.6f}, std: {current_hidden.std():.6f}")
    
    last_worker = worker_stubs[-1]
    if last_worker["stub"] is None:
        # Local processing
        with mx.stream(mx.gpu):
            current_hidden = model.model.norm(current_hidden)
            # Most MLX models use tied embeddings
            if hasattr(model.model, 'embed_tokens') and hasattr(model.model.embed_tokens, 'as_linear'):
                # Use tied embeddings (most common case)
                logits = model.model.embed_tokens.as_linear(current_hidden)
            elif hasattr(model, 'lm_head') and model.lm_head is not None:
                # Use separate lm_head if available
                logits = model.lm_head(current_hidden)
            else:
                logger.error(f"Cannot find output projection. Model type: {type(model)}")
                raise AttributeError("Cannot find output projection layer")
            mx.eval(logits)
    else:
        # Remote processing
        request = inference_pb2.ForwardRequest()
        request.input_tensor.CopyFrom(serialize_tensor(current_hidden))
        request.is_final_projection = True
        
        response = last_worker["stub"].Forward(request)
        logits = deserialize_tensor(response.output)
    
    logger.info(f"DEBUG: Final logits - shape: {logits.shape}, mean: {logits.mean():.6f}, std: {logits.std():.6f}")
    
    # DEBUG: Show top tokens for last position
    try:
        last_position_logits = logits[0, -1, :]
        # Get indices of top 5 values
        top_indices = mx.argpartition(-last_position_logits, 5)[:5]
        top_values = last_position_logits[top_indices]
        sorted_idx = mx.argsort(-top_values)
        top_indices = top_indices[sorted_idx]
        top_values = top_values[sorted_idx]
        
        logger.info(f"DEBUG: Top 5 tokens - indices: {top_indices.tolist()}, values: {top_values.tolist()}")
        
        # Decode top tokens to see what they are
        top_tokens = [tokenizer.decode([int(idx)]) for idx in top_indices.tolist()]
        logger.info(f"DEBUG: Top 5 tokens decoded: {top_tokens}")
        
        # Check if logits are reasonable
        logit_std = last_position_logits.std()
        logit_min = last_position_logits.min()
        logit_max = last_position_logits.max()
        logger.info(f"DEBUG: Logit stats - std: {logit_std:.3f}, min: {logit_min:.3f}, max: {logit_max:.3f}")
    except Exception as e:
        logger.info(f"DEBUG: Error getting top tokens: {e}")
    
    # Monitor memory at end
    mem_info = mx.metal.device_info() or {}
    end_memory = mem_info.get('used_memory', 0) / 1e9
    memory_delta = end_memory - start_memory
    
    elapsed = time.time() - total_start
    tokens_per_second = input_ids.shape[1] / elapsed if elapsed > 0 else 0
    
    logger.info(f"Total forward pass took {elapsed:.3f}s ({tokens_per_second:.1f} tok/s) - Memory delta: {memory_delta:.2f}GB")
    return logits

def generate_single_device(prompt: str, max_tokens: int, temperature: float) -> str:
    """Generate text using single device with proper KV caching."""
    # Use MLX's built-in generate function which handles KV caching properly
    from mlx_lm.sample_utils import make_sampler
    sampler = make_sampler(temp=temperature)
    return generate(model, tokenizer, prompt=prompt, max_tokens=max_tokens, sampler=sampler)

async def generate_distributed(prompt: str, max_tokens: int, temperature: float) -> tuple[str, dict]:
    """Generate text using distributed model."""
    # Check sequence length to decide strategy
    prompt_tokens = tokenizer.encode(prompt)
    
    # Decide whether to use distributed or single-device
    # Read environment variable dynamically to allow per-request override
    use_distributed_setting = os.getenv('USE_DISTRIBUTED_INFERENCE', DEFAULT_USE_DISTRIBUTED_INFERENCE)
    use_distributed = False
    
    if use_distributed_setting == 'always':
        use_distributed = True
    elif use_distributed_setting == 'never':
        use_distributed = False
    else:  # 'auto' - ALWAYS USE DISTRIBUTED
        # This is a distributed inference system - we MUST use it
        # The whole point is to scale to models that don't fit on one device
        use_distributed = True
    
    if not use_distributed:
        logger.info(f"Using single-device generation with KV cache (prompt_len={len(prompt_tokens)}, max_tokens={max_tokens})")
        start_time = time.time()
        result = generate_single_device(prompt, max_tokens, temperature)
        elapsed = time.time() - start_time
        
        # Calculate performance metrics
        completion_tokens = len(tokenizer.encode(result)) - len(prompt_tokens)
        metrics = {
            "prompt_eval_time": 0.1,  # Estimate for single-device
            "eval_time": elapsed - 0.1,
            "prompt_tokens": len(prompt_tokens),
            "completion_tokens": completion_tokens
        }
        return result, metrics
    
    # Distributed inference with KV caching
    logger.info(f"ðŸ”¥ DISTRIBUTED MODE WITH KV CACHE - 2 DEVICES (prompt_len={len(prompt_tokens)}, max_tokens={max_tokens})")
    logger.info(f"ðŸ“± Device 1 (mini1): Layers 0-13 with KV cache")
    logger.info(f"ðŸ–¥ï¸  Device 2 (mini2): Layers 14-27 with KV cache")
    
    from mlx_lm.sample_utils import make_sampler
    sampler = make_sampler(temp=temperature)
    
    # Generate session ID for this generation
    import uuid
    session_id = str(uuid.uuid4())
    
    # Start with the prompt
    input_ids = mx.array(prompt_tokens).reshape(1, -1)
    generated_tokens = []
    
    logger.info(f"Starting distributed generation with KV cache - session {session_id}")
    
    # Track timing for metrics
    prompt_eval_start = time.time()
    
    # Initial forward pass for the PROMPT (creates KV cache)
    logits = await distributed_forward(input_ids, session_id=session_id, is_prompt=True)
    prompt_eval_time = time.time() - prompt_eval_start
    logger.info(f"Prompt processing took {prompt_eval_time:.3f}s for {len(prompt_tokens)} tokens")
    
    eval_start = time.time()
    
    # Get last token from initial forward
    with mx.stream(mx.gpu):
        last_logits = logits[0, -1:, :]
        
        # DEBUG: Check logits before sampling
        logger.info(f"DEBUG: Initial logits shape: {last_logits.shape}, mean: {last_logits.mean():.3f}, std: {last_logits.std():.3f}")
        
        next_token = sampler(last_logits)
        mx.eval(next_token)
        next_token_id = int(next_token[0].item())
        
        # DEBUG: Decode the token to see what it is
        token_text = tokenizer.decode([next_token_id])
        logger.info(f"DEBUG: Initial sampled token {next_token_id} = '{token_text}'")
    
    generated_tokens.append(next_token_id)
    logger.info(f"Step 1: Generated token {next_token_id}")
    
    if next_token_id != tokenizer.eos_token_id and max_tokens > 1:
        # Generate remaining tokens with INCREMENTAL passes (O(n) with KV cache)
        for step in range(1, max_tokens):
            # CRITICAL: Only pass the LAST generated token for incremental generation
            # The KV cache maintains the context from previous tokens
            last_token_tensor = mx.array([[next_token_id]])  # Shape: [1, 1]
            
            # Incremental forward pass with KV cache (only processes 1 token!)
            logits = await distributed_forward(last_token_tensor, session_id=session_id, is_prompt=False)
            
            # Sample next token
            with mx.stream(mx.gpu):
                # Get logits for the single token we just processed
                last_logits = logits[0, -1:, :]  # Keep batch dimension
                
                # Sample using MLX's sampler
                next_token = sampler(last_logits)
                mx.eval(next_token)
                
                next_token_id = int(next_token[0].item())
                
                # DEBUG: Log generation progress
                if step % 10 == 0:
                    token_text = tokenizer.decode([next_token_id])
                    logger.info(f"Step {step+1}: Generated token {next_token_id} = '{token_text}'")
            
            generated_tokens.append(next_token_id)
            
            # Stop if EOS token
            if next_token_id == tokenizer.eos_token_id:
                logger.info(f"Hit EOS token, stopping generation at {step+1} tokens")
                break
    
    eval_time = time.time() - eval_start
    
    # Decode the same way as single-device
    generated_text = tokenizer.decode(generated_tokens)
    full_text = prompt + generated_text
    
    logger.info(f"Distributed generation complete: {len(generated_tokens)} tokens generated")
    
    # Calculate performance metrics
    metrics = {
        "prompt_eval_time": prompt_eval_time,
        "eval_time": eval_time,
        "prompt_tokens": len(prompt_tokens),
        "completion_tokens": len(generated_tokens)
    }
    
    return full_text, metrics

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest) -> ChatResponse:
    """OpenAI-compatible chat completions endpoint."""
    try:
        # Format messages using Qwen3's proper chat template
        formatted_messages = []
        for msg in request.messages:
            formatted_messages.append({"role": msg.role, "content": msg.content})
        
        # Use the tokenizer's apply_chat_template method for proper formatting
        prompt = tokenizer.apply_chat_template(
            formatted_messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # Generate response with metrics
        start_time = time.time()
        response_text, metrics = await generate_distributed(prompt, request.max_tokens, request.temperature)
        elapsed = time.time() - start_time
        
        # Extract just the assistant's response
        if response_text.startswith(prompt):
            response_text = response_text[len(prompt):]
        
        # Clean up Qwen3 special tokens and extra whitespace
        response_text = response_text.replace('<|im_end|>', '').strip()
        
        # Calculate tokens/second metrics
        prompt_eval_tokens_per_second = metrics["prompt_tokens"] / metrics["prompt_eval_time"] if metrics["prompt_eval_time"] > 0 else 0
        eval_tokens_per_second = metrics["completion_tokens"] / metrics["eval_time"] if metrics["eval_time"] > 0 else 0
        
        logger.info(f"Generated {metrics['completion_tokens']} tokens in {elapsed:.2f}s")
        logger.info(f"Prompt eval: {prompt_eval_tokens_per_second:.1f} tok/s, Generation: {eval_tokens_per_second:.1f} tok/s")
        
        # Return OpenAI-compatible response with performance metrics
        return ChatResponse(
            id=f"chatcmpl-{os.urandom(8).hex()}",
            created=int(time.time()),
            model="mlx-community/Qwen3-1.7B-8bit",
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }],
            usage={
                "prompt_tokens": metrics["prompt_tokens"],
                "completion_tokens": metrics["completion_tokens"],
                "total_tokens": metrics["prompt_tokens"] + metrics["completion_tokens"],
                "prompt_eval_tokens_per_second": round(prompt_eval_tokens_per_second, 1),
                "eval_tokens_per_second": round(eval_tokens_per_second, 1)
            }
        )
        
    except Exception as e:
        logger.error(f"Chat completion error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """Health check endpoint."""
    worker_status = []
    for i, worker_info in enumerate(worker_stubs):
        if worker_info["stub"] is None:
            worker_status.append({"worker": i, "status": "local", "layers": f"{worker_info['assignment']['start_layer']}-{worker_info['assignment']['end_layer']-1}"})
        else:
            try:
                response = worker_info["stub"].HealthCheck(inference_pb2.HealthRequest())
                worker_status.append({"worker": i, "status": response.status, "layers": f"{worker_info['assignment']['start_layer']}-{worker_info['assignment']['end_layer']-1}"})
            except:
                worker_status.append({"worker": i, "status": "offline", "layers": f"{worker_info['assignment']['start_layer']}-{worker_info['assignment']['end_layer']-1}"})
    
    return {
        "status": "healthy",
        "model": "mlx-community/Qwen3-1.7B-8bit",
        "workers": worker_status
    }

# Also run local worker gRPC service
class LocalWorkerService(inference_pb2_grpc.InferenceServiceServicer):
    """Local worker service for processing layers on coordinator."""
    
    def ProcessLayers(self, request, context):
        """Process transformer layers locally with KV cache support."""
        try:
            session_id = request.session_id or "default"
            is_prompt = request.is_prompt
            
            logger.info(f"LocalWorkerService.ProcessLayers - session={session_id}, is_prompt={is_prompt}, layers={request.start_layer}-{request.end_layer}")
            
            # Handle cache clearing
            if request.clear_cache and session_id in local_kv_caches:
                del local_kv_caches[session_id]
                logger.info(f"Cleared local cache for session {session_id}")
            
            input_tensor = deserialize_tensor(request.input_tensor)
            
            # Get or create KV cache for this session
            if session_id not in local_kv_caches:
                local_kv_caches[session_id] = {}
                logger.info(f"Created new local KV cache for session {session_id}")
            
            session_cache = local_kv_caches[session_id]
            
            # Handle attention mask based on pass type
            T = input_tensor.shape[1]
            if is_prompt and T > 1:
                # For prompt processing, use full causal mask
                if T not in mask_cache:
                    mask_cache[T] = create_causal_mask(T, offset=0)
                attention_mask = mask_cache[T]
            else:
                # For incremental generation, no mask needed (single token)
                attention_mask = None
            
            with mx.stream(mx.gpu):
                hidden = input_tensor
                
                for idx, i in enumerate(range(request.start_layer, request.end_layer)):
                    layer_key = f"layer_{i}"
                    
                    if is_prompt or layer_key not in session_cache:
                        # First pass or no cache - create new KVCache object
                        layer_cache = KVCache()
                        session_cache[layer_key] = layer_cache
                    else:
                        # Use existing cache for incremental generation
                        layer_cache = session_cache[layer_key]
                    
                    # Process layer with cache (cache is updated in-place)
                    hidden = model.model.layers[i](hidden, attention_mask, layer_cache)
                
                mx.eval(hidden)
                mx.synchronize()  # CRITICAL: Ensure all operations complete before serialization
            
            response = inference_pb2.LayerResponseV2()
            response.output_tensor.CopyFrom(serialize_tensor(hidden))
            return response
            
        except Exception as e:
            logger.error(f"ProcessLayers error: {e}")
            context.abort(grpc.StatusCode.INTERNAL, str(e))
    
    def HealthCheck(self, request, context):
        """Health check."""
        return inference_pb2.HealthResponse(status="healthy")
    
    def AllReduce(self, request, context):
        """Handle AllReduce operations for tensor parallelism."""
        try:
            operation = request.operation
            session_id = request.session_id
            device_id = request.device_id
            
            logger.info(f"AllReduce operation: {operation} from device {device_id} for session {session_id}")
            
            # Deserialize input tensor
            input_tensor = deserialize_tensor(request.tensor)
            
            # For now, implement simple gather-broadcast pattern
            # This is called on the coordinator (device 0)
            if operation == "GATHER":
                # Store the tensor from the worker
                if not hasattr(self, '_allreduce_buffers'):
                    self._allreduce_buffers = {}
                
                if session_id not in self._allreduce_buffers:
                    self._allreduce_buffers[session_id] = {}
                
                self._allreduce_buffers[session_id][device_id] = input_tensor
                
                # Check if we have all tensors
                if len(self._allreduce_buffers[session_id]) == request.world_size - 1:
                    # We have all worker tensors, now sum them
                    # Note: coordinator's tensor is handled separately
                    tensors = list(self._allreduce_buffers[session_id].values())
                    result = mx.sum(mx.stack(tensors), axis=0)
                    
                    # Clean up buffer
                    del self._allreduce_buffers[session_id]
                    
                    # Return summed result
                    response = inference_pb2.AllReduceResponse()
                    response.result_tensor.CopyFrom(serialize_tensor(result))
                    response.status = "completed"
                    return response
                else:
                    # Still waiting for other devices
                    response = inference_pb2.AllReduceResponse()
                    response.status = "pending"
                    return response
                    
            elif operation == "BROADCAST":
                # Broadcast the result back to workers
                response = inference_pb2.AllReduceResponse()
                response.result_tensor.CopyFrom(serialize_tensor(input_tensor))
                response.status = "completed"
                return response
                
            else:
                context.abort(grpc.StatusCode.UNIMPLEMENTED, f"Operation {operation} not implemented")
                
        except Exception as e:
            logger.error(f"AllReduce error: {e}")
            context.abort(grpc.StatusCode.INTERNAL, str(e))
    
    def Forward(self, request, context):
        """Handle special forward passes (embeddings, final projection)."""
        try:
            if request.is_embedding:
                # Process embedding layer
                input_ids = mx.array(request.input_ids).reshape(1, -1)
                
                with mx.stream(mx.gpu):
                    embeddings = model.model.embed_tokens(input_ids)
                    mx.eval(embeddings)
                    mx.synchronize()  # Ensure operations complete before serialization
                
                response = inference_pb2.ForwardResponse()
                response.output.CopyFrom(serialize_tensor(embeddings))
                return response
                
            elif request.is_final_projection:
                # Process final norm and projection
                input_tensor = deserialize_tensor(request.input_tensor)
                
                with mx.stream(mx.gpu):
                    # Apply final layer norm
                    normed = model.model.norm(input_tensor)
                    # Project to vocabulary
                    # Most MLX models use tied embeddings
                    if hasattr(model.model, 'embed_tokens') and hasattr(model.model.embed_tokens, 'as_linear'):
                        # Use tied embeddings (most common case)
                        logits = model.model.embed_tokens.as_linear(normed)
                    elif hasattr(model, 'lm_head') and model.lm_head is not None:
                        # Use separate lm_head if available
                        logits = model.lm_head(normed)
                    else:
                        logger.error(f"Cannot find output projection in LocalWorkerService")
                        raise AttributeError("Cannot find output projection layer")
                    mx.eval(logits)
                    mx.synchronize()  # Ensure operations complete before serialization
                
                response = inference_pb2.ForwardResponse()
                response.output.CopyFrom(serialize_tensor(logits))
                return response
                
            else:
                context.abort(grpc.StatusCode.INVALID_ARGUMENT, "Unknown forward request type")
                
        except Exception as e:
            logger.error(f"Forward error: {e}", exc_info=True)
            context.abort(grpc.StatusCode.INTERNAL, str(e))

async def serve_grpc():
    """Serve gRPC service for local worker."""
    server = grpc.aio.server(futures.ThreadPoolExecutor(max_workers=10))
    inference_pb2_grpc.add_InferenceServiceServicer_to_server(LocalWorkerService(), server)
    server.add_insecure_port('[::]:50051')
    await server.start()
    logger.info("Local gRPC worker service started on port 50051")
    await server.wait_for_termination()

async def main():
    """Main entry point."""
    # Initialize coordinator
    initialize_coordinator()
    
    # Start both gRPC and FastAPI servers
    grpc_task = asyncio.create_task(serve_grpc())
    
    # Start FastAPI
    config = uvicorn.Config(
        app=app,
        host="0.0.0.0",
        port=8100,
        log_level="info"
    )
    server = uvicorn.Server(config)
    
    logger.info("Starting FastAPI server on port 8100")
    await server.serve()

if __name__ == "__main__":
    asyncio.run(main())