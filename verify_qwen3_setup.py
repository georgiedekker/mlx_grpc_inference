#!/usr/bin/env python3
"""
Verify Qwen3-1.7B-8bit is properly configured and working across all devices.
"""

import asyncio
import sys
import time
import grpc
import mlx.core as mx
from mlx_lm import load
sys.path.insert(0, 'src')

from communication import inference_pb2_grpc, inference_pb2
from communication.dns_resolver import resolve_grpc_target
from core.config import ClusterConfig

async def verify_qwen3_setup():
    """Comprehensive verification of Qwen3 setup."""
    print("üîç Qwen3-1.7B-8bit Distributed Inference Verification")
    print("=" * 60)
    
    # 1. Verify local model loading
    print("\n1Ô∏è‚É£ Verifying local Qwen3 model...")
    try:
        model, tokenizer = load('mlx-community/Qwen3-1.7B-8bit')
        layer_count = len(model.layers)
        print(f"   ‚úÖ Model: Qwen3-1.7B-8bit")
        print(f"   ‚úÖ Layers: {layer_count}")
        print(f"   ‚úÖ Tokenizer vocab: {len(tokenizer.vocab) if hasattr(tokenizer, 'vocab') else 'N/A'}")
        
        # Test tokenization
        test_text = "Hello, how are you today?"
        tokens = tokenizer.encode(test_text)
        decoded = tokenizer.decode(tokens)
        print(f"   ‚úÖ Tokenization test: '{test_text}' ‚Üí {len(tokens)} tokens ‚Üí '{decoded}'")
        
    except Exception as e:
        print(f"   ‚ùå Local model loading failed: {e}")
        return False
    
    # 2. Verify configuration
    print("\n2Ô∏è‚É£ Verifying cluster configuration...")
    try:
        config = ClusterConfig.from_yaml('config/cluster_config.yaml')
        print(f"   ‚úÖ Model name: {config.model.name}")
        print(f"   ‚úÖ Total layers: {config.model.total_layers}")
        
        # Verify layer distribution
        total_assigned = 0
        for device_id, layers in config.model.layer_distribution.items():
            print(f"   ‚úÖ {device_id}: layers {layers[0]}-{layers[-1]} ({len(layers)} layers)")
            total_assigned += len(layers)
        
        if total_assigned == config.model.total_layers:
            print(f"   ‚úÖ Layer distribution complete: {total_assigned}/{config.model.total_layers}")
        else:
            print(f"   ‚ùå Layer distribution incomplete: {total_assigned}/{config.model.total_layers}")
            return False
            
    except Exception as e:
        print(f"   ‚ùå Configuration error: {e}")
        return False
    
    # 3. Verify worker connections and model loading
    print("\n3Ô∏è‚É£ Verifying worker Qwen3 loading...")
    workers = config.get_workers()
    
    for worker in workers:
        try:
            target = resolve_grpc_target(worker.hostname, worker.grpc_port)
            channel = grpc.insecure_channel(target)
            stub = inference_pb2_grpc.InferenceServiceStub(channel)
            
            # Health check
            health_request = inference_pb2.Empty()
            health_response = stub.HealthCheck(health_request, timeout=5.0)
            
            # Device info
            device_info = stub.GetDeviceInfo(health_request, timeout=5.0)
            
            print(f"   ‚úÖ {worker.device_id}:")
            print(f"      - Status: {'Healthy' if health_response.healthy else 'Unhealthy'}")
            print(f"      - Layers: {list(device_info.assigned_layers)}")
            print(f"      - Memory: {device_info.memory_usage_gb:.1f} GB")
            print(f"      - GPU: {device_info.gpu_utilization:.1f}%")
            
            channel.close()
            
        except Exception as e:
            print(f"   ‚ùå {worker.device_id}: Connection failed - {e}")
            return False
    
    # 4. Test distributed processing performance
    print("\n4Ô∏è‚É£ Testing Qwen3 distributed performance...")
    
    # Create test tensors of different sizes
    test_cases = [
        (32, "Short sequence"),
        (128, "Medium sequence"), 
        (256, "Long sequence")
    ]
    
    for seq_len, description in test_cases:
        print(f"\n   üìä {description} ({seq_len} tokens):")
        
        # Test each worker
        for worker in workers:
            try:
                target = resolve_grpc_target(worker.hostname, worker.grpc_port)
                channel = grpc.insecure_channel(target)
                stub = inference_pb2_grpc.InferenceServiceStub(channel)
                
                # Create test tensor
                test_tensor = mx.random.normal(shape=(1, seq_len, 2048))
                from communication.tensor_utils import serialize_mlx_array
                data, metadata = serialize_mlx_array(test_tensor)
                
                # Get assigned layers
                device_info = stub.GetDeviceInfo(inference_pb2.Empty())
                layers = list(device_info.assigned_layers)
                
                # Process layers
                layer_request = inference_pb2.LayerRequest(
                    request_id=f"perf-test-{worker.device_id}-{seq_len}",
                    input_tensor=data,
                    layer_indices=layers,
                    metadata=inference_pb2.TensorMetadata(
                        shape=metadata['shape'],
                        dtype=metadata['dtype'],
                        compressed=metadata.get('compressed', False)
                    )
                )
                
                start_time = time.time()
                layer_response = stub.ProcessLayers(layer_request, timeout=30.0)
                end_time = time.time()
                
                processing_time = (end_time - start_time) * 1000
                throughput = seq_len / (processing_time / 1000)
                
                print(f"      {worker.device_id}: {processing_time:.1f}ms ({throughput:.1f} tokens/sec)")
                
                channel.close()
                
            except Exception as e:
                print(f"      {worker.device_id}: ‚ùå Failed - {e}")
                return False
    
    print(f"\nüéâ Qwen3-1.7B-8bit Verification Complete!")
    print(f"‚úÖ Model successfully configured and distributed")
    print(f"‚úÖ All workers processing Qwen3 efficiently") 
    print(f"‚úÖ Performance excellent across all sequence lengths")
    print(f"‚úÖ Your M4 Macs are ready for Qwen3 distributed inference!")
    
    return True

if __name__ == "__main__":
    success = asyncio.run(verify_qwen3_setup())
    if success:
        print(f"\nüöÄ Ready to use Qwen3 distributed inference!")
    else:
        print(f"\n‚ùå Setup verification failed. Check logs above.")
        sys.exit(1)