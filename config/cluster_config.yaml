# MLX Distributed Inference Cluster Configuration

cluster:
  name: "mlx-inference-cluster"
  coordinator_device_id: "mini1"
  communication_backend: "grpc"
  
devices:
  - device_id: "mini1"
    hostname: "mini1.local"
    api_port: 8100
    grpc_port: 50051
    role: "coordinator"
    rank: 0
    capabilities:
      model: "Apple M4"
      memory_gb: 16
      gpu_cores: 10
      cpu_cores: 10
      cpu_performance_cores: 4
      cpu_efficiency_cores: 6
      neural_engine_cores: 16
      bandwidth_gbps: 120
      mlx_metal_available: true
      max_recommended_model_size_gb: 10
    
  - device_id: "mini2"
    hostname: "mini2.local"
    api_port: 8101
    grpc_port: 50051
    role: "worker"
    rank: 1
    capabilities:
      model: "Apple M4"
      memory_gb: 16
      gpu_cores: 10
      cpu_cores: 10
      cpu_performance_cores: 4
      cpu_efficiency_cores: 6
      neural_engine_cores: 16
      bandwidth_gbps: 120
      mlx_metal_available: true
      max_recommended_model_size_gb: 10
    
  - device_id: "master"
    hostname: "m4.local"
    api_port: 8102
    grpc_port: 50051
    role: "worker"
    rank: 2
    ssh_user: "georgedekker"
    ssh_key: "~/.ssh/mlx_master_key"
    capabilities:
      model: "Apple M4"
      memory_gb: 48
      gpu_cores: 10
      cpu_cores: 10
      cpu_performance_cores: 4
      cpu_efficiency_cores: 6
      neural_engine_cores: 16
      bandwidth_gbps: 120
      mlx_metal_available: true
      max_recommended_model_size_gb: 30

model:
  name: "mlx-community/Qwen3-1.7B-8bit"
  total_layers: 28
  layer_distribution:
    mini1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]        # 10 layers
    mini2: [10, 11, 12, 13, 14, 15, 16, 17, 18]  # 9 layers
    master: [19, 20, 21, 22, 23, 24, 25, 26, 27] # 9 layers

performance:
  batch_size: 1
  max_sequence_length: 2048
  tensor_compression: true
  compression_algorithm: "lz4"
  connection_pool_size: 10
  request_timeout_seconds: 60
  heartbeat_interval_seconds: 5
  enable_kv_cache: true
  parallel_worker_processing: true

monitoring:
  enable_gpu_monitoring: true
  enable_metrics_export: true
  metrics_port: 9090
  log_level: "INFO"