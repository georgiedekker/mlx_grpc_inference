# Test cluster configuration for distributed MLX inference
devices:
  - device_id: "coordinator"
    hostname: "localhost"
    rank: 0
    role: "coordinator"
    grpc_port: 50051
    api_port: 8000
  
  - device_id: "worker_1"
    hostname: "localhost"
    rank: 1
    role: "worker"
    grpc_port: 50052
    api_port: 8001
  
  - device_id: "worker_2"
    hostname: "localhost"
    rank: 2
    role: "worker"
    grpc_port: 50053
    api_port: 8002

model:
  name: "test_model"
  path: "mlx-community/Qwen2.5-1.5B-Instruct-4bit"
  layer_distribution:
    coordinator: [0, 1, 2]
    worker_1: [3, 4, 5]
    worker_2: [6, 7, 8]
  max_sequence_length: 2048
  context_window: 4096

performance:
  request_timeout_seconds: 30.0
  max_concurrent_requests: 10
  tensor_compression: true
  memory_limit_gb: 8.0