/Users/mini1/Movies/mlx_grpc_inference/tensor_parallel_server.py:168: DeprecationWarning: 
        on_event is deprecated, use lifespan event handlers instead.

        Read more about it in the
        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
        
  @app.on_event("startup")
INFO:     Started server process [40531]
INFO:     Waiting for application startup.
INFO:__main__:Loading model for tensor parallel sharding...
Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 134337.14it/s]
INFO:__main__:Sharding model weights across 2 devices...
INFO:src.tensor_parallel:Sharded model into 2 parts
INFO:src.tensor_parallel:Device 0: 252 weight tensors
INFO:src.tensor_parallel:Device 1: 252 weight tensors
INFO:__main__:Connected to worker on mini2
INFO:__main__:Tensor parallel coordinator initialized
INFO:__main__:API server initialized with tensor parallel coordinator
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8100 (Press CTRL+C to quit)
INFO:     127.0.0.1:59357 - "GET /health HTTP/1.1" 200 OK
INFO:__main__:Processing prompt with temperature=0.7, max_tokens=100
INFO:__main__:Generated 100 tokens in 2.10s (~49.5 tok/s overall)
INFO:     127.0.0.1:59384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:59442 - "GET /docs HTTP/1.1" 200 OK
INFO:     127.0.0.1:59442 - "GET /openapi.json HTTP/1.1" 200 OK
INFO:__main__:Processing prompt with temperature=0.7, max_tokens=100
ERROR:__main__:Error processing request: Either input_embeddings or prompt (or both) must be provided.
INFO:     127.0.0.1:59445 - "POST /v1/chat/completions HTTP/1.1" 500 Internal Server Error
INFO:__main__:Processing prompt with temperature=0.7, max_tokens=100
INFO:__main__:Generated 100 tokens in 2.05s (~50.3 tok/s overall)
INFO:     127.0.0.1:59448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:__main__:Processing prompt with temperature=0.7, max_tokens=100
INFO:__main__:Generated 100 tokens in 2.12s (~49.1 tok/s overall)
INFO:     127.0.0.1:59450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:__main__:Processing prompt with temperature=0.7, max_tokens=100
INFO:__main__:Generated 100 tokens in 2.11s (~60.2 tok/s overall)
INFO:     127.0.0.1:59452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [40531]
/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py:301: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown: {'/mp-oun3pwfa'}
  warnings.warn(
