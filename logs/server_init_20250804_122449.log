2025-08-04 12:24:49,614 - __main__ - INFO - Logging initialized: logs/server_init_20250804_122449.log
2025-08-04 12:24:49,615 - __main__ - INFO - Rank 0: Starting API server on port 8100
2025-08-04 12:24:49,635 - __main__ - INFO - Starting Distributed Inference Server
2025-08-04 12:24:49,635 - __main__ - INFO - Set default device to GPU
2025-08-04 12:24:49,635 - __main__ - INFO - Backend initialization order: ['mpi', 'ring', 'any']
2025-08-04 12:24:49,635 - __main__ - INFO - Trying backend: mpi
2025-08-04 12:24:49,716 - __main__ - INFO - [Rank 0] Logging initialized: logs/server_rank0_20250804_122449.log
2025-08-04 12:24:49,716 - __main__ - INFO - [Rank 0] Initialized: Backend=mpi, Rank=0/1
2025-08-04 12:24:49,716 - __main__ - INFO - [Rank 0] Environment variables:
2025-08-04 12:24:49,716 - __main__ - INFO - [Rank 0]   MLX_PINNED_ALLOC=1
2025-08-04 12:24:49,716 - __main__ - INFO - [Rank 0]   MLX_RING_IFACE=bridge0
2025-08-04 12:24:49,716 - __main__ - INFO - [Rank 0]   MLX_RING_PORT=50000
2025-08-04 12:24:49,716 - __main__ - INFO - [Rank 0]   MLX_RING_RANK=0
2025-08-04 12:24:49,716 - __main__ - INFO - [Rank 0]   MLX_RING_SIZE=2
2025-08-04 12:24:49,716 - __main__ - INFO - [Rank 0]   MODEL_NAME=mlx-community/Qwen3-1.7B-8bit
2025-08-04 12:24:49,716 - __main__ - INFO - [Rank 0] Loading model: mlx-community/Qwen3-1.7B-8bit
2025-08-04 12:24:49,718 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-08-04 12:24:49,896 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "GET /api/models/mlx-community/Qwen3-1.7B-8bit/revision/main HTTP/1.1" 200 6006
2025-08-04 12:24:51,261 - __main__ - INFO - [Rank 0] Model loaded successfully
2025-08-04 12:25:22,772 - __main__ - INFO - Shutting down...
2025-08-04 12:25:22,773 - __main__ - INFO - Received signal 15, shutting down...
