2025-08-04 15:16:19,555 - Rank ? - INFO - Running in single-node mode
2025-08-04 15:16:19,555 - Rank ? - INFO - Creating inference engine for mlx-community/Qwen3-1.7B-8bit
2025-08-04 15:16:19,571 - Rank 0 - INFO - Initializing model shard on mps
2025-08-04 15:16:20,194 - Rank 0 - INFO - Loading model: mlx-community/Qwen3-1.7B-8bit
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 252, in load_mlx_model_for_pytorch
    logger.info(f"Loading MLX model {model_name} for PyTorch")
Message: 'Loading MLX model mlx-community/Qwen3-1.7B-8bit for PyTorch'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 108, in convert_mlx_to_pytorch
    logger.info(f"Converting MLX model from {model_path}")
Message: 'Converting MLX model from /Users/mini1/.cache/huggingface/hub/models--mlx-community--Qwen3-1.7B-8bit/snapshots/8c24f6782a91421513803ce527a27dcc560ab904'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 117, in convert_mlx_to_pytorch
    mlx_weights = MLXModelAdapter.load_mlx_weights(model_path)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 57, in load_mlx_weights
    logger.info(f"Found weight files: {[f.name for f in weight_files]}")
Message: "Found weight files: ['model.safetensors']"
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.embed_tokens.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.embed_tokens.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.embed_tokens.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.0.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.0.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.0.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.0.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.0.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.0.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.0.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.1.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.1.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.1.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.1.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.1.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.1.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.1.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.10.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.10.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.10.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.10.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.10.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.10.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.10.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.11.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.11.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.11.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.11.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.11.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.11.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.11.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.12.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.12.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.12.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.12.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.12.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.12.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.12.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.13.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.13.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.13.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.13.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.13.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.13.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.13.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.14.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.14.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.14.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.14.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.14.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.14.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.14.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.15.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.15.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.15.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.15.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.15.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.15.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.15.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.16.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.16.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.16.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.16.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.16.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.16.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.16.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.17.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.17.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.17.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.17.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.17.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.17.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.17.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.18.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.18.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.18.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.18.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.18.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.18.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.18.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.19.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.19.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.19.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.19.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.19.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.19.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.19.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.2.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.2.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.2.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.2.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.2.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.2.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.2.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.20.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.20.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.20.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.20.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.20.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.20.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.20.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.21.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.21.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.21.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.21.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.21.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.21.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.21.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.22.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.22.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.22.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.22.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.22.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.22.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.22.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.23.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.23.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.23.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.23.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.23.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.23.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.23.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.24.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.24.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.24.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.24.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.24.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.24.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.24.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.25.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.25.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.25.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.25.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.25.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.25.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.25.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.26.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.26.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.26.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.26.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.26.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.26.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.26.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.27.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.27.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.27.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.27.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.27.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.27.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.27.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.3.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.3.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.3.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.3.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.3.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.3.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.3.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.4.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.4.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.4.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.4.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.4.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.4.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.4.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.5.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.5.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.5.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.5.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.5.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.5.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.5.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.6.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.6.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.6.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.6.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.6.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.6.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.6.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.7.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.7.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.7.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.7.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.7.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.7.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.7.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.8.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.8.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.8.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.8.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.8.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.8.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.8.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.input_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.down_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.down_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.9.mlp.down_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.gate_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.gate_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.9.mlp.gate_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.up_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.up_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.9.mlp.up_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.post_attention_layernorm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.9.self_attn.k_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.o_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.o_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.9.self_attn.o_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.9.self_attn.q_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.v_proj.biases, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.v_proj.scales, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.uint32 for model.layers.9.self_attn.v_proj.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 126, in convert_mlx_to_pytorch
    dequantized = MLXModelAdapter.dequantize_mlx_weight(weight, name, quant_config)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 101, in dequantize_mlx_weight
    logger.warning(f"Unknown weight dtype {weight.dtype} for {weight_name}, converting to float16")
Message: 'Unknown weight dtype torch.bfloat16 for model.norm.weight, converting to float16'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 257, in load_mlx_model_for_pytorch
    weights, config = MLXModelAdapter.convert_mlx_to_pytorch(model_name)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 130, in convert_mlx_to_pytorch
    logger.info(f"Converted {len(pytorch_weights)} weights")
Message: 'Converted 704 weights'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 194, in _load_weights
    logger.warning(f"Shape mismatch for {pytorch_key}: expected {expected_shape}, got {actual_shape}")
Message: 'Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 472, in format
    return self._format(record)
           ~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 468, in _format
    return self._fmt % values
           ~~~~~~~~~~^~~~~~~~
KeyError: 'rank'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 1151, in emit
    msg = self.format(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 715, in format
    s = self.formatMessage(record)
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 684, in formatMessage
    return self._style.format(record)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py", line 474, in format
    raise ValueError('Formatting field not found in record: %s' % e)
ValueError: Formatting field not found in record: 'rank'
Call stack:
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 393, in <module>
    main()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 314, in main
    engine = DistributedInferenceEngine(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 138, in __init__
    self.model_shard = DistributedModelShard(model_name, rank, world_size)
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 54, in __init__
    self._load_and_shard_model()
  File "/Users/mini1/Movies/mlx_grpc_inference/pytorch_distributed_server.py", line 64, in _load_and_shard_model
    self.model, _ = load_mlx_model_for_pytorch(self.model_name, self.device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 258, in load_mlx_model_for_pytorch
    model = MLXCompatibleModel(config, weights, device=device)
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 148, in __init__
    self._load_weights()
  File "/Users/mini1/Movies/mlx_grpc_inference/src/utils/mlx_pytorch_adapter.py", line 199, in _load_weights
    logger.info(f"Loaded {len(loaded_keys)} weights, missing {len(missing_keys)}")
Message: 'Loaded 113 weights, missing 198'
Arguments: ()
2025-08-04 15:16:47,058 - Rank 0 - INFO - Total layers: 28, assigned layers: 0-27
2025-08-04 15:16:47,059 - Rank 0 - INFO - Model shard initialized with 28 layers
2025-08-04 15:16:47,064 - Rank 0 - INFO - Rank 0: Starting API server on port 8100
