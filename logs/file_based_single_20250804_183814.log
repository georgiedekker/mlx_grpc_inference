2025-08-04 18:38:16,461 - [__main__] - INFO - Parsed environment: RANK=0, WORLD_SIZE=1
2025-08-04 18:38:16,469 - [src.coordination.file_based_coordinator] - INFO - Detected Thunderbolt IP: 192.168.5.1
2025-08-04 18:38:16,469 - [src.coordination.file_based_coordinator] - INFO - Initialized file-based coordinator: rank=0, world_size=1
2025-08-04 18:38:16,469 - [__main__] - INFO - Initialized file-based server: rank=0, world_size=1
2025-08-04 18:38:16,469 - [__main__] - INFO - Initializing file-based coordination...
2025-08-04 18:38:16,470 - [src.coordination.file_based_coordinator] - INFO - Waiting for 1 nodes to register...
2025-08-04 18:38:16,470 - [src.coordination.file_based_coordinator] - INFO - All nodes registered and alive
2025-08-04 18:38:16,472 - [src.coordination.file_based_coordinator] - INFO - Assigned layers: {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]}
2025-08-04 18:38:16,472 - [src.coordination.file_based_coordinator] - INFO - Loading layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
2025-08-04 18:38:18,483 - [src.coordination.file_based_coordinator] - INFO - Loaded 28 layers
2025-08-04 18:38:18,485 - [src.coordination.file_based_coordinator] - INFO - Distributed inference system initialized successfully
2025-08-04 18:38:18,485 - [__main__] - INFO - Loading device capabilities...
2025-08-04 18:38:18,496 - [__main__] - INFO - Loading assigned model layers...
2025-08-04 18:38:18,496 - [__main__] - INFO - Loading layers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
2025-08-04 18:38:18,507 - [__main__] - INFO - Loading MLX model: mlx-community/Qwen3-1.7B-8bit
2025-08-04 18:38:18,524 - [__main__] - INFO - Using device: mps
2025-08-04 18:38:18,525 - [src.utils.mlx_pytorch_adapter] - INFO - Loading MLX model mlx-community/Qwen3-1.7B-8bit for PyTorch
2025-08-04 18:38:18,526 - [src.utils.mlx_pytorch_adapter] - INFO - Converting MLX model from /Users/mini1/.cache/huggingface/hub/models--mlx-community--Qwen3-1.7B-8bit/snapshots/8c24f6782a91421513803ce527a27dcc560ab904
2025-08-04 18:38:18,533 - [src.utils.mlx_pytorch_adapter] - INFO - Found weight files: ['model.safetensors']
2025-08-04 18:38:18,645 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.embed_tokens.biases, converting to float16
2025-08-04 18:38:18,655 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.embed_tokens.scales, converting to float16
2025-08-04 18:38:18,664 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.embed_tokens.weight, converting to float16
2025-08-04 18:38:18,965 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.input_layernorm.weight, converting to float16
2025-08-04 18:38:18,966 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:18,966 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:18,967 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:18,980 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:18,981 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:18,981 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:18,994 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:18,994 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:18,995 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,004 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,004 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,004 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,004 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,004 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,006 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,006 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,007 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,011 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,011 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,011 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,011 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,015 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,015 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,015 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,017 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,017 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,018 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,018 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,031 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,031 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,032 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,040 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,041 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,041 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,054 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,054 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,054 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,054 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,054 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,056 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,056 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,056 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,064 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,064 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,064 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,066 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,066 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,067 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,068 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,080 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,081 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,081 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,094 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,094 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,095 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,107 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,107 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,107 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,107 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,107 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,109 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,109 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,109 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,117 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,117 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,117 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,119 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,119 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,119 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,120 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,133 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,133 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,134 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,142 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,143 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,143 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,152 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,152 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,152 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,152 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,152 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,154 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,154 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,154 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,158 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,158 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,158 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,158 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,162 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,162 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,162 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,164 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,164 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,165 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,165 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,178 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,178 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,179 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,191 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,192 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,192 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,205 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,205 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,205 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,205 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,205 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,207 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,207 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,207 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,211 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,211 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,211 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,211 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,215 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,215 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,215 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,217 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,217 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,217 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,218 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,230 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,231 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,232 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,244 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,245 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,246 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,258 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,258 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,258 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,258 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,259 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,260 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,260 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,261 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,265 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,265 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,265 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,265 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,269 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,269 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,269 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,271 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,271 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,271 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,272 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,284 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,284 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,285 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,298 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,298 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,299 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,311 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,311 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,311 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,311 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,311 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,313 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,313 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,313 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,316 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,316 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,316 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,316 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,320 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,320 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,320 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,322 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,322 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,323 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,324 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,335 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,335 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,336 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,350 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,350 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,351 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,359 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,359 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,359 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,359 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,359 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,361 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,361 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,361 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,365 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,365 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,365 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,366 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,369 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,369 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,369 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,371 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,371 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,372 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,373 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,386 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,386 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,387 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,400 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,400 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,401 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,412 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,412 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,412 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,412 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,412 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,414 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,414 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,414 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,418 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,418 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,418 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,418 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,422 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,422 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,422 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,424 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,424 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,424 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,425 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,434 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,434 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,435 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,447 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,448 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,448 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,461 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,461 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,461 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,461 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,461 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,463 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,463 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,463 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,467 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,467 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,467 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,467 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,472 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,472 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,472 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,474 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,474 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,474 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,475 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,488 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,489 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,489 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,500 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,501 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,502 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,513 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,513 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,513 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,513 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,513 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,515 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,516 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,516 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,520 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,520 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,520 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,520 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,524 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,524 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,524 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,526 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,526 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,526 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,527 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,539 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,540 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,540 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,552 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,553 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,553 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,565 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,565 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,565 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,566 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,566 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,567 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,567 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,567 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,571 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,571 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,571 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,571 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,575 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,575 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,575 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,577 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,577 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,578 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,578 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,591 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,591 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,592 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,604 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,605 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,605 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,618 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,618 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,618 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,618 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,618 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,620 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,620 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,620 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,624 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,624 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,624 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,625 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,628 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,628 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,629 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,631 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,631 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,631 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,644 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,645 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,645 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,658 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,659 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,660 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,674 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,674 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,674 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,679 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,679 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,679 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,679 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,684 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,684 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,684 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,686 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,686 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,686 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,687 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,699 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,700 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,700 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,709 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,709 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,710 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,720 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,720 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,720 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,720 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,720 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,722 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,722 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,722 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,726 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,726 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,726 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,726 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,730 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,730 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,730 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,732 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,732 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,733 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,733 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,746 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,746 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,747 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,759 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,760 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,760 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,773 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,773 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,773 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,773 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,773 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,775 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,775 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,775 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,779 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,779 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,779 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,779 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,783 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,783 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,783 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,785 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,785 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,785 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,786 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,799 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,799 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,800 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,812 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,813 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,813 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,825 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,825 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,825 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,825 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,826 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,828 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,828 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,828 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,836 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,836 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,836 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,838 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,838 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,838 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,838 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,851 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,851 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,852 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,864 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,865 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,866 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,876 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,876 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,876 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,879 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,879 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,879 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,879 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,883 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,883 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,883 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,885 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,885 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,886 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,887 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,899 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,900 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,900 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,914 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,914 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,923 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,923 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,923 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,923 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,923 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,933 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,933 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,933 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,935 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,935 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,935 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,936 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:19,948 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:19,949 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:19,950 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:19,962 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:19,963 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:19,963 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:19,975 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:19,975 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:19,975 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:19,975 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:19,975 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:19,977 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:19,977 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:19,977 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:19,982 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:19,982 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:19,982 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:19,982 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:19,986 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:19,986 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:19,986 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:19,988 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.input_layernorm.weight, converting to float16
2025-08-04 18:38:19,988 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:19,988 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:19,989 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:20,002 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:20,002 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:20,003 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:20,015 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:20,015 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:20,016 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:20,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:20,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:20,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:20,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:20,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:20,030 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:20,030 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:20,030 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:20,034 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:20,034 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:20,034 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:20,034 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:20,038 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:20,038 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:20,038 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:20,040 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.input_layernorm.weight, converting to float16
2025-08-04 18:38:20,040 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:20,040 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:20,041 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:20,053 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:20,053 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:20,054 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:20,066 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:20,067 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:20,068 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:20,080 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:20,080 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:20,080 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:20,080 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:20,080 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:20,082 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:20,082 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:20,082 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:20,086 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:20,086 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:20,086 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:20,086 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:20,091 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:20,091 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:20,091 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:20,092 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.input_layernorm.weight, converting to float16
2025-08-04 18:38:20,092 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:20,093 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:20,093 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:20,106 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:20,106 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:20,107 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:20,119 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:20,120 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:20,121 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:20,129 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:20,129 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:20,129 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:20,129 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:20,129 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:20,131 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:20,131 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:20,131 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:20,136 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:20,136 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:20,136 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:20,136 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:20,140 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:20,140 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:20,140 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:20,142 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.input_layernorm.weight, converting to float16
2025-08-04 18:38:20,142 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:20,142 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:20,143 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:20,155 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:20,156 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:20,156 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:20,169 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:20,169 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:20,170 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:20,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:20,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:20,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:20,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:20,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:20,185 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:20,185 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:20,185 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:20,190 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:20,190 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:20,190 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:20,190 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:20,194 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:20,194 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:20,194 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:20,196 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.input_layernorm.weight, converting to float16
2025-08-04 18:38:20,196 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:20,196 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:20,197 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:20,209 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:20,210 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:20,210 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:20,222 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:20,223 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:20,223 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:20,235 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:20,235 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:20,235 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:20,235 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:20,235 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:20,237 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:20,237 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:20,237 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:20,242 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:20,242 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:20,242 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:20,242 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:20,246 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:20,246 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:20,246 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:20,247 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.input_layernorm.weight, converting to float16
2025-08-04 18:38:20,247 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:20,248 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:20,248 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:20,260 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:20,261 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:20,261 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:20,274 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:20,274 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:20,275 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:20,288 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:20,288 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:20,288 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:20,288 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:20,288 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:20,290 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:20,290 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:20,290 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:20,294 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:20,294 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:20,294 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:20,294 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:20,298 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:20,298 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:20,298 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:20,300 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.input_layernorm.weight, converting to float16
2025-08-04 18:38:20,300 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:20,301 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:20,301 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:20,312 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:20,312 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:20,313 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:20,323 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:20,324 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:20,324 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:20,337 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:20,337 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:20,337 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:20,337 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:20,337 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:20,339 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:20,339 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:20,339 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:20,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:20,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:20,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:20,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:20,346 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:20,346 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:20,346 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:20,348 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.input_layernorm.weight, converting to float16
2025-08-04 18:38:20,348 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.down_proj.biases, converting to float16
2025-08-04 18:38:20,348 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.down_proj.scales, converting to float16
2025-08-04 18:38:20,349 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.down_proj.weight, converting to float16
2025-08-04 18:38:20,357 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.gate_proj.biases, converting to float16
2025-08-04 18:38:20,358 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.gate_proj.scales, converting to float16
2025-08-04 18:38:20,358 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.gate_proj.weight, converting to float16
2025-08-04 18:38:20,371 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.up_proj.biases, converting to float16
2025-08-04 18:38:20,372 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.up_proj.scales, converting to float16
2025-08-04 18:38:20,372 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.up_proj.weight, converting to float16
2025-08-04 18:38:20,384 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.post_attention_layernorm.weight, converting to float16
2025-08-04 18:38:20,384 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_norm.weight, converting to float16
2025-08-04 18:38:20,384 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_proj.biases, converting to float16
2025-08-04 18:38:20,385 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_proj.scales, converting to float16
2025-08-04 18:38:20,385 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.k_proj.weight, converting to float16
2025-08-04 18:38:20,387 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.o_proj.biases, converting to float16
2025-08-04 18:38:20,387 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.o_proj.scales, converting to float16
2025-08-04 18:38:20,387 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.o_proj.weight, converting to float16
2025-08-04 18:38:20,391 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_norm.weight, converting to float16
2025-08-04 18:38:20,391 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_proj.biases, converting to float16
2025-08-04 18:38:20,391 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_proj.scales, converting to float16
2025-08-04 18:38:20,391 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.q_proj.weight, converting to float16
2025-08-04 18:38:20,395 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.v_proj.biases, converting to float16
2025-08-04 18:38:20,395 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.v_proj.scales, converting to float16
2025-08-04 18:38:20,395 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.v_proj.weight, converting to float16
2025-08-04 18:38:20,397 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.norm.weight, converting to float16
2025-08-04 18:38:20,397 - [src.utils.mlx_pytorch_adapter] - INFO - Converted 704 weights
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,872 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:38:39,875 - [src.utils.mlx_pytorch_adapter] - INFO - Loaded 113 weights, missing 198
2025-08-04 18:38:41,197 - [__main__] - INFO - Successfully loaded model with 28 assigned layers
2025-08-04 18:38:41,197 - [__main__] - INFO - Initializing KV cache...
2025-08-04 18:38:41,197 - [src.core.pytorch_kv_cache] - INFO - Device mini1: 716 sequences, 11468.8MB budget, 100.0% compute share
2025-08-04 18:38:41,197 - [__main__] - ERROR - Failed to initialize KV cache: DistributedKVCacheManager.__init__() got an unexpected keyword argument 'cache_size'
2025-08-04 18:38:41,197 - [__main__] - ERROR - Server initialization failed: DistributedKVCacheManager.__init__() got an unexpected keyword argument 'cache_size'
2025-08-04 18:38:41,197 - [__main__] - ERROR - Server initialization failed
2025-08-04 18:38:41,197 - [__main__] - INFO - Shutting down server...
2025-08-04 18:38:41,198 - [src.coordination.file_based_coordinator] - INFO - Cleaned up coordination files
