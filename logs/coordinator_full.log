INFO:     Started server process [7068]
INFO:     Waiting for application startup.
INFO:__main__:ðŸš€ Starting MLX Distributed Inference API...
INFO:__main__:âœ… Config loaded: mlx-community/Qwen3-1.7B-8bit
INFO:__main__:ðŸ“¦ Loading model and tokenizer...
Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]Fetching 9 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 109416.63it/s]
INFO:__main__:âœ… Model loaded: 28 layers
INFO:__main__:ðŸ”— Connecting to workers...
INFO:communication.dns_resolver:Resolved mini2.local -> 192.168.2.15 using mdns_native in 3.1ms
INFO:__main__:âœ… mini2: Connected (9 layers)
INFO:communication.dns_resolver:Resolved m4.local -> 192.168.5.3 using mdns_native in 1.4ms
INFO:__main__:âœ… master: Connected (9 layers)
INFO:__main__:âœ… API server ready!
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8100 (Press CTRL+C to quit)
INFO:     127.0.0.1:52611 - "GET /docs HTTP/1.1" 200 OK
INFO:     127.0.0.1:52611 - "GET /openapi.json HTTP/1.1" 200 OK
INFO:     127.0.0.1:52613 - "GET /apple-touch-icon-precomposed.png HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:52613 - "GET /apple-touch-icon.png HTTP/1.1" 404 Not Found
INFO:__main__:Hidden states before workers - shape: (1, 19, 2048), dtype: mlx.core.bfloat16
INFO:__main__:Prefill: Processing through 2 workers
INFO:__main__:Received response from mini2 in 174.3ms
INFO:__main__:Received response from master in 80.4ms
INFO:__main__:Model doesn't have lm_head, checking for tied embeddings...
INFO:__main__:Used tied embeddings (embed_tokens.as_linear) - logits shape: (1, 19, 151936)
INFO:__main__:Performance: 19 prompt tokens @ 51.9 tok/s, 30 generated @ 7.1 tok/s, total time: 4.59s
INFO:     127.0.0.1:52677 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:__main__:Hidden states before workers - shape: (1, 19, 2048), dtype: mlx.core.bfloat16
INFO:__main__:Prefill: Processing through 2 workers
INFO:__main__:Received response from mini2 in 87.1ms
INFO:__main__:Received response from master in 62.3ms
INFO:__main__:Model doesn't have lm_head, checking for tied embeddings...
INFO:__main__:Used tied embeddings (embed_tokens.as_linear) - logits shape: (1, 19, 151936)
INFO:__main__:Performance: 19 prompt tokens @ 98.7 tok/s, 10 generated @ 7.0 tok/s, total time: 1.62s
INFO:     127.0.0.1:52742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:__main__:Hidden states before workers - shape: (1, 8, 2048), dtype: mlx.core.bfloat16
INFO:__main__:Prefill: Processing through 2 workers
INFO:__main__:Received response from mini2 in 110.5ms
INFO:__main__:Received response from master in 52.6ms
INFO:__main__:Model doesn't have lm_head, checking for tied embeddings...
INFO:__main__:Used tied embeddings (embed_tokens.as_linear) - logits shape: (1, 8, 151936)
INFO:__main__:Performance: 8 prompt tokens @ 27.1 tok/s, 50 generated @ 6.7 tok/s, total time: 7.81s
INFO:     127.0.0.1:52787 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:__main__:Hidden states before workers - shape: (1, 8, 2048), dtype: mlx.core.bfloat16
INFO:__main__:Prefill: Processing through 2 workers
INFO:__main__:Received response from mini2 in 85.6ms
INFO:__main__:Received response from master in 58.8ms
INFO:__main__:Model doesn't have lm_head, checking for tied embeddings...
INFO:__main__:Used tied embeddings (embed_tokens.as_linear) - logits shape: (1, 8, 151936)
INFO:__main__:Performance: 8 prompt tokens @ 40.1 tok/s, 50 generated @ 6.8 tok/s, total time: 7.59s
INFO:     127.0.0.1:52791 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:__main__:Hidden states before workers - shape: (1, 8, 2048), dtype: mlx.core.bfloat16
INFO:__main__:Prefill: Processing through 2 workers
INFO:__main__:Received response from mini2 in 95.7ms
INFO:__main__:Received response from master in 52.7ms
INFO:__main__:Model doesn't have lm_head, checking for tied embeddings...
INFO:__main__:Used tied embeddings (embed_tokens.as_linear) - logits shape: (1, 8, 151936)
INFO:__main__:Performance: 8 prompt tokens @ 26.9 tok/s, 50 generated @ 6.7 tok/s, total time: 7.77s
INFO:     127.0.0.1:52795 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:53095 - "GET /docs HTTP/1.1" 200 OK
INFO:     127.0.0.1:53095 - "GET /openapi.json HTTP/1.1" 200 OK
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:__main__:ðŸ‘‹ Shutting down API server...
INFO:     Application shutdown complete.
INFO:     Finished server process [7068]
/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py:301: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown: {'/mp-y_o_q0sh'}
  warnings.warn(
