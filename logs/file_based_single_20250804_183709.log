2025-08-04 18:37:11,502 - [__main__] - INFO - Parsed environment: RANK=0, WORLD_SIZE=1
2025-08-04 18:37:11,510 - [src.coordination.file_based_coordinator] - INFO - Detected Thunderbolt IP: 192.168.5.1
2025-08-04 18:37:11,510 - [src.coordination.file_based_coordinator] - INFO - Initialized file-based coordinator: rank=0, world_size=1
2025-08-04 18:37:11,510 - [__main__] - INFO - Initialized file-based server: rank=0, world_size=1
2025-08-04 18:37:11,510 - [__main__] - INFO - Initializing file-based coordination...
2025-08-04 18:37:11,510 - [src.coordination.file_based_coordinator] - INFO - Waiting for 1 nodes to register...
2025-08-04 18:37:11,510 - [src.coordination.file_based_coordinator] - INFO - All nodes registered and alive
2025-08-04 18:37:11,513 - [src.coordination.file_based_coordinator] - INFO - Assigned layers: {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]}
2025-08-04 18:37:11,513 - [src.coordination.file_based_coordinator] - INFO - Loading layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
2025-08-04 18:37:13,518 - [src.coordination.file_based_coordinator] - INFO - Loaded 28 layers
2025-08-04 18:37:13,520 - [src.coordination.file_based_coordinator] - INFO - Distributed inference system initialized successfully
2025-08-04 18:37:13,520 - [__main__] - INFO - Loading device capabilities...
2025-08-04 18:37:13,530 - [__main__] - INFO - Loading assigned model layers...
2025-08-04 18:37:13,531 - [__main__] - INFO - Loading layers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
2025-08-04 18:37:13,539 - [__main__] - INFO - Loading MLX model: mlx-community/Qwen3-1.7B-8bit
2025-08-04 18:37:13,539 - [__main__] - INFO - Using device: mps
2025-08-04 18:37:13,539 - [src.utils.mlx_pytorch_adapter] - INFO - Loading MLX model mlx-community/Qwen3-1.7B-8bit for PyTorch
2025-08-04 18:37:13,540 - [src.utils.mlx_pytorch_adapter] - INFO - Converting MLX model from /Users/mini1/.cache/huggingface/hub/models--mlx-community--Qwen3-1.7B-8bit/snapshots/8c24f6782a91421513803ce527a27dcc560ab904
2025-08-04 18:37:13,548 - [src.utils.mlx_pytorch_adapter] - INFO - Found weight files: ['model.safetensors']
2025-08-04 18:37:13,643 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.embed_tokens.biases, converting to float16
2025-08-04 18:37:13,658 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.embed_tokens.scales, converting to float16
2025-08-04 18:37:13,673 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.embed_tokens.weight, converting to float16
2025-08-04 18:37:14,080 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.input_layernorm.weight, converting to float16
2025-08-04 18:37:14,081 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:14,081 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:14,082 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:14,096 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:14,097 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:14,098 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:14,119 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:14,119 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:14,120 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:14,136 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:14,136 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:14,136 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:14,136 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:14,136 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:14,140 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:14,140 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:14,140 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:14,147 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:14,147 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:14,147 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:14,148 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:14,154 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:14,154 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:14,154 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:14,156 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.input_layernorm.weight, converting to float16
2025-08-04 18:37:14,156 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:14,156 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:14,156 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:14,174 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:14,174 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:14,175 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:14,192 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:14,193 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:14,194 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:14,211 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:14,211 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:14,211 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:14,211 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:14,212 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:14,216 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:14,217 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:14,217 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:14,224 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:14,224 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:14,224 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:14,225 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:14,229 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:14,229 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:14,229 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:14,232 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.input_layernorm.weight, converting to float16
2025-08-04 18:37:14,232 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:14,233 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:14,234 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:14,252 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:14,253 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:14,254 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:14,275 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:14,276 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:14,276 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:14,294 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:14,294 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:14,294 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:14,294 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:14,294 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:14,297 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:14,297 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:14,297 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:14,302 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:14,302 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:14,303 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:14,303 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:14,310 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:14,311 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:14,311 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:14,314 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.input_layernorm.weight, converting to float16
2025-08-04 18:37:14,314 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:14,315 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:14,316 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:14,335 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:14,336 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:14,337 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:14,353 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:14,353 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:14,354 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:14,369 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:14,369 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:14,369 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:14,369 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:14,369 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:14,374 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:14,374 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:14,374 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:14,383 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:14,383 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:14,383 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:14,383 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:14,389 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:14,389 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:14,389 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:14,393 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.input_layernorm.weight, converting to float16
2025-08-04 18:37:14,393 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:14,394 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:14,395 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:14,414 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:14,415 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:14,416 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:14,436 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:14,437 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:14,438 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:14,453 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:14,453 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:14,453 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:14,453 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:14,454 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:14,458 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:14,458 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:14,458 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:14,464 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:14,464 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:14,464 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:14,465 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:14,469 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:14,469 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:14,469 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:14,473 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.input_layernorm.weight, converting to float16
2025-08-04 18:37:14,473 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:14,474 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:14,475 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:14,493 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:14,494 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:14,495 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:14,512 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:14,513 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:14,513 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:14,531 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:14,531 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:14,531 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:14,531 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:14,531 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:14,535 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:14,536 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:14,536 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:14,542 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:14,542 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:14,542 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:14,543 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:14,551 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:14,551 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:14,552 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:14,556 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.input_layernorm.weight, converting to float16
2025-08-04 18:37:14,556 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:14,557 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:14,557 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:14,575 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:14,576 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:14,577 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:14,597 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:14,597 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:14,598 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:14,618 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:14,618 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:14,618 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:14,618 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:14,618 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:14,622 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:14,623 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:14,623 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:14,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:14,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:14,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:14,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:14,639 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:14,639 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:14,639 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:14,641 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.input_layernorm.weight, converting to float16
2025-08-04 18:37:14,641 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:14,643 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:14,644 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:14,661 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:14,662 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:14,662 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:14,683 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:14,684 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:14,684 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:14,700 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:14,700 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:14,700 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:14,700 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:14,700 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:14,705 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:14,706 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:14,707 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:14,715 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:14,715 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:14,715 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:14,715 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:14,723 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:14,723 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:14,723 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:14,727 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.input_layernorm.weight, converting to float16
2025-08-04 18:37:14,727 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:14,728 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:14,729 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:14,748 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:14,749 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:14,750 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:14,766 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:14,767 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:14,768 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:14,787 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:14,787 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:14,787 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:14,788 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:14,788 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:14,792 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:14,793 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:14,793 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:14,799 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:14,799 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:14,800 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:14,800 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:14,806 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:14,806 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:14,807 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:14,810 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.input_layernorm.weight, converting to float16
2025-08-04 18:37:14,810 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:14,811 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:14,812 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:14,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:14,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:14,834 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:14,849 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:14,849 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:14,850 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:14,866 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:14,866 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:14,866 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:14,866 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:14,866 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:14,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:14,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:14,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:14,878 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:14,878 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:14,878 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:14,879 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:14,887 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:14,887 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:14,888 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:14,892 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.input_layernorm.weight, converting to float16
2025-08-04 18:37:14,892 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:14,894 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:14,895 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:14,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:14,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:14,914 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:14,932 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:14,933 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:14,934 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:14,954 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:14,954 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:14,954 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:14,954 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:14,955 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:14,959 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:14,959 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:14,960 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:14,964 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:14,964 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:14,964 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:14,965 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:14,970 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:14,971 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:14,971 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:14,975 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.input_layernorm.weight, converting to float16
2025-08-04 18:37:14,975 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:14,976 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:14,977 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:14,994 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:14,995 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:14,996 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:15,013 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:15,014 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:15,015 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:15,035 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:15,035 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:15,035 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:15,035 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:15,035 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:15,038 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:15,038 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:15,039 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:15,046 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:15,046 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:15,046 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:15,046 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:15,052 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:15,052 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:15,052 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:15,056 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.input_layernorm.weight, converting to float16
2025-08-04 18:37:15,056 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:15,057 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:15,057 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:15,075 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:15,075 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:15,076 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:15,094 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:15,095 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:15,096 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:15,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:15,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:15,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:15,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:15,114 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:15,116 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:15,117 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:15,117 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:15,125 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:15,125 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:15,125 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:15,125 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:15,131 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:15,131 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:15,132 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:15,135 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.input_layernorm.weight, converting to float16
2025-08-04 18:37:15,135 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:15,137 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:15,137 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:15,157 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:15,157 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:15,158 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:15,179 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:15,180 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:15,180 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:15,199 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:15,199 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:15,199 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:15,199 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:15,199 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:15,202 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:15,203 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:15,204 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:15,211 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:15,211 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:15,211 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:15,212 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:15,217 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:15,217 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:15,217 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:15,221 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.input_layernorm.weight, converting to float16
2025-08-04 18:37:15,221 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:15,223 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:15,223 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:15,241 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:15,242 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:15,244 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:15,261 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:15,262 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:15,262 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:15,280 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:15,280 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:15,280 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:15,280 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:15,280 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:15,284 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:15,285 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:15,285 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:15,291 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:15,291 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:15,291 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:15,291 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:15,297 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:15,298 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:15,298 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:15,302 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.input_layernorm.weight, converting to float16
2025-08-04 18:37:15,302 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:15,303 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:15,303 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:15,321 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:15,322 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:15,322 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:15,342 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:15,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:15,344 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:15,362 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:15,362 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:15,362 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:15,362 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:15,362 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:15,365 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:15,366 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:15,366 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:15,372 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:15,372 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:15,372 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:15,372 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:15,377 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:15,378 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:15,378 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:15,380 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.input_layernorm.weight, converting to float16
2025-08-04 18:37:15,380 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:15,380 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:15,381 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:15,402 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:15,403 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:15,404 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:15,424 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:15,425 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:15,426 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:15,449 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:15,449 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:15,449 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:15,449 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:15,449 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:15,454 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:15,454 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:15,455 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:15,461 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:15,461 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:15,462 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:15,462 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:15,469 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:15,469 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:15,470 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:15,474 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.input_layernorm.weight, converting to float16
2025-08-04 18:37:15,474 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:15,475 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:15,476 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:15,497 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:15,499 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:15,500 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:15,521 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:15,522 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:15,523 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:15,542 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:15,542 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:15,542 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:15,542 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:15,543 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:15,547 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:15,547 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:15,547 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:15,553 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:15,553 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:15,554 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:15,554 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:15,560 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:15,560 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:15,560 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:15,564 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.input_layernorm.weight, converting to float16
2025-08-04 18:37:15,564 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:15,565 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:15,565 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:15,584 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:15,584 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:15,585 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:15,602 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:15,603 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:15,603 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:15,617 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:15,617 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:15,617 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:15,617 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:15,617 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:15,622 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:15,622 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:15,623 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:15,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:15,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:15,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:15,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:15,636 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:15,637 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:15,637 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:15,639 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.input_layernorm.weight, converting to float16
2025-08-04 18:37:15,639 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:15,640 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:15,640 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:15,658 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:15,659 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:15,660 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:15,686 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:15,688 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:15,689 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:15,715 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:15,716 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:15,716 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:15,716 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:15,716 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:15,722 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:15,722 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:15,723 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:15,733 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:15,733 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:15,734 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:15,734 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:15,758 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:15,758 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:15,758 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:15,763 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.input_layernorm.weight, converting to float16
2025-08-04 18:37:15,763 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:15,764 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:15,765 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:15,791 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:15,793 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:15,794 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:15,816 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:15,817 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:15,817 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:15,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:15,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:15,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:15,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:15,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:15,836 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:15,837 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:15,838 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:15,846 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:15,846 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:15,846 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:15,846 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:15,850 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:15,850 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:15,850 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:15,852 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.input_layernorm.weight, converting to float16
2025-08-04 18:37:15,852 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:15,853 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:15,854 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:15,869 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:15,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:15,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:15,886 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:15,887 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:15,889 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:15,901 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:15,901 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:15,901 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:15,901 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:15,901 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:15,906 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:15,906 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:15,907 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:15,915 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:15,915 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:15,915 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:15,915 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:15,921 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:15,922 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:15,922 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:15,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.input_layernorm.weight, converting to float16
2025-08-04 18:37:15,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:15,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:15,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:15,948 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:15,948 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:15,949 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:15,971 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:15,972 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:15,973 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:15,991 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:15,991 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:15,991 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:15,991 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:15,991 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:15,996 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:15,997 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:15,998 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:16,006 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:16,006 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:16,007 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:16,007 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:16,015 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:16,016 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:16,016 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:16,020 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.input_layernorm.weight, converting to float16
2025-08-04 18:37:16,020 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:16,022 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:16,022 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:16,041 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:16,042 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:16,043 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:16,062 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:16,063 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:16,064 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:16,085 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:16,085 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:16,085 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:16,085 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:16,085 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:16,088 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:16,089 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:16,089 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:16,095 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:16,095 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:16,096 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:16,096 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:16,105 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:16,106 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:16,106 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:16,109 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.input_layernorm.weight, converting to float16
2025-08-04 18:37:16,109 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:16,110 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:16,112 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:16,135 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:16,135 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:16,136 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:16,156 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:16,157 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:16,158 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:16,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:16,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:16,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:16,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:16,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:16,188 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:16,189 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:16,189 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:16,196 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:16,196 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:16,197 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:16,198 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:16,204 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:16,205 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:16,205 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:16,208 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.input_layernorm.weight, converting to float16
2025-08-04 18:37:16,208 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:16,209 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:16,210 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:16,229 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:16,231 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:16,232 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:16,253 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:16,255 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:16,255 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:16,277 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:16,277 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:16,277 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:16,277 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:16,277 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:16,279 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:16,280 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:16,280 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:16,287 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:16,287 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:16,287 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:16,287 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:16,294 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:16,294 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:16,294 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:16,298 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.input_layernorm.weight, converting to float16
2025-08-04 18:37:16,298 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:16,298 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:16,299 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:16,315 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:16,316 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:16,316 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:16,337 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:16,339 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:16,340 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:16,363 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:16,363 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:16,363 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:16,363 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:16,363 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:16,366 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:16,367 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:16,368 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:16,376 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:16,376 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:16,377 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:16,377 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:16,384 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:16,384 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:16,384 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:16,387 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.input_layernorm.weight, converting to float16
2025-08-04 18:37:16,387 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.down_proj.biases, converting to float16
2025-08-04 18:37:16,388 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.down_proj.scales, converting to float16
2025-08-04 18:37:16,389 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.down_proj.weight, converting to float16
2025-08-04 18:37:16,410 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.gate_proj.biases, converting to float16
2025-08-04 18:37:16,412 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.gate_proj.scales, converting to float16
2025-08-04 18:37:16,413 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.gate_proj.weight, converting to float16
2025-08-04 18:37:16,435 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.up_proj.biases, converting to float16
2025-08-04 18:37:16,436 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.up_proj.scales, converting to float16
2025-08-04 18:37:16,436 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.up_proj.weight, converting to float16
2025-08-04 18:37:16,457 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.post_attention_layernorm.weight, converting to float16
2025-08-04 18:37:16,457 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_norm.weight, converting to float16
2025-08-04 18:37:16,457 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_proj.biases, converting to float16
2025-08-04 18:37:16,457 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_proj.scales, converting to float16
2025-08-04 18:37:16,458 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.k_proj.weight, converting to float16
2025-08-04 18:37:16,460 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.o_proj.biases, converting to float16
2025-08-04 18:37:16,460 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.o_proj.scales, converting to float16
2025-08-04 18:37:16,460 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.o_proj.weight, converting to float16
2025-08-04 18:37:16,464 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_norm.weight, converting to float16
2025-08-04 18:37:16,464 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_proj.biases, converting to float16
2025-08-04 18:37:16,465 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_proj.scales, converting to float16
2025-08-04 18:37:16,465 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.q_proj.weight, converting to float16
2025-08-04 18:37:16,469 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.v_proj.biases, converting to float16
2025-08-04 18:37:16,469 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.v_proj.scales, converting to float16
2025-08-04 18:37:16,469 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.v_proj.weight, converting to float16
2025-08-04 18:37:16,471 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.norm.weight, converting to float16
2025-08-04 18:37:16,471 - [src.utils.mlx_pytorch_adapter] - INFO - Converted 704 weights
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,927 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:37:35,929 - [src.utils.mlx_pytorch_adapter] - INFO - Loaded 113 weights, missing 198
2025-08-04 18:37:37,285 - [__main__] - INFO - Successfully loaded model with 28 assigned layers
2025-08-04 18:37:37,285 - [__main__] - INFO - Initializing KV cache...
2025-08-04 18:37:37,285 - [src.core.pytorch_kv_cache] - INFO - Device mini1: 716 sequences, 11468.8MB budget, 100.0% compute share
2025-08-04 18:37:37,285 - [__main__] - ERROR - Failed to initialize KV cache: DistributedKVCacheManager.__init__() got an unexpected keyword argument 'cache_size'
2025-08-04 18:37:37,285 - [__main__] - ERROR - Server initialization failed: DistributedKVCacheManager.__init__() got an unexpected keyword argument 'cache_size'
2025-08-04 18:37:37,285 - [__main__] - ERROR - Server initialization failed
2025-08-04 18:37:37,285 - [__main__] - INFO - Shutting down server...
2025-08-04 18:37:37,286 - [src.coordination.file_based_coordinator] - INFO - Cleaned up coordination files
