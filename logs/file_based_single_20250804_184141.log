2025-08-04 18:41:43,041 - [__main__] - INFO - Parsed environment: RANK=0, WORLD_SIZE=1
2025-08-04 18:41:43,049 - [src.coordination.file_based_coordinator] - INFO - Detected Thunderbolt IP: 192.168.5.1
2025-08-04 18:41:43,049 - [src.coordination.file_based_coordinator] - INFO - Initialized file-based coordinator: rank=0, world_size=1
2025-08-04 18:41:43,049 - [__main__] - INFO - Initialized file-based server: rank=0, world_size=1
2025-08-04 18:41:43,049 - [__main__] - INFO - Initializing file-based coordination...
2025-08-04 18:41:43,050 - [src.coordination.file_based_coordinator] - INFO - Waiting for 1 nodes to register...
2025-08-04 18:41:43,050 - [src.coordination.file_based_coordinator] - INFO - All nodes registered and alive
2025-08-04 18:41:43,052 - [src.coordination.file_based_coordinator] - INFO - Assigned layers: {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]}
2025-08-04 18:41:43,052 - [src.coordination.file_based_coordinator] - INFO - Loading layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
2025-08-04 18:41:45,063 - [src.coordination.file_based_coordinator] - INFO - Loaded 28 layers
2025-08-04 18:41:45,065 - [src.coordination.file_based_coordinator] - INFO - Distributed inference system initialized successfully
2025-08-04 18:41:45,065 - [__main__] - INFO - Loading device capabilities...
2025-08-04 18:41:45,075 - [__main__] - INFO - Loading assigned model layers...
2025-08-04 18:41:45,076 - [__main__] - INFO - Loading layers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
2025-08-04 18:41:45,082 - [__main__] - INFO - Loading MLX model: mlx-community/Qwen3-1.7B-8bit
2025-08-04 18:41:45,082 - [__main__] - INFO - Using device: mps
2025-08-04 18:41:45,082 - [src.utils.mlx_pytorch_adapter] - INFO - Loading MLX model mlx-community/Qwen3-1.7B-8bit for PyTorch
2025-08-04 18:41:45,083 - [src.utils.mlx_pytorch_adapter] - INFO - Converting MLX model from /Users/mini1/.cache/huggingface/hub/models--mlx-community--Qwen3-1.7B-8bit/snapshots/8c24f6782a91421513803ce527a27dcc560ab904
2025-08-04 18:41:45,090 - [src.utils.mlx_pytorch_adapter] - INFO - Found weight files: ['model.safetensors']
2025-08-04 18:41:45,204 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.embed_tokens.biases, converting to float16
2025-08-04 18:41:45,214 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.embed_tokens.scales, converting to float16
2025-08-04 18:41:45,224 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.embed_tokens.weight, converting to float16
2025-08-04 18:41:45,516 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.input_layernorm.weight, converting to float16
2025-08-04 18:41:45,517 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:45,517 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:45,518 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:45,530 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:45,531 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:45,531 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:45,544 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:45,544 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:45,545 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:45,553 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:45,553 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:45,553 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:45,553 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:45,553 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:45,555 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:45,555 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:45,556 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:45,560 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:45,560 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:45,560 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:45,560 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:45,564 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:45,564 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:45,564 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:45,566 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.input_layernorm.weight, converting to float16
2025-08-04 18:41:45,566 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:45,566 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:45,566 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:45,579 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:45,579 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:45,580 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:45,589 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:45,589 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:45,590 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:45,603 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:45,603 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:45,603 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:45,603 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:45,603 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:45,605 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:45,605 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:45,605 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:45,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:45,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:45,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:45,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:45,614 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:45,614 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:45,614 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:45,616 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.input_layernorm.weight, converting to float16
2025-08-04 18:41:45,616 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:45,616 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:45,617 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:45,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:45,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:45,631 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:45,643 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:45,644 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:45,644 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:45,656 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:45,656 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:45,656 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:45,656 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:45,656 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:45,658 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:45,658 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:45,658 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:45,662 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:45,662 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:45,662 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:45,662 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:45,666 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:45,666 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:45,666 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:45,667 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.input_layernorm.weight, converting to float16
2025-08-04 18:41:45,667 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:45,668 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:45,669 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:45,681 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:45,682 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:45,682 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:45,690 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:45,691 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:45,691 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:45,700 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:45,700 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:45,700 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:45,700 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:45,700 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:45,702 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:45,702 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:45,702 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:45,706 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:45,706 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:45,706 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:45,706 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:45,710 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:45,710 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:45,710 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:45,712 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.input_layernorm.weight, converting to float16
2025-08-04 18:41:45,712 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:45,712 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:45,713 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:45,724 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:45,725 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:45,725 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:45,738 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:45,738 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:45,739 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:45,752 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:45,752 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:45,752 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:45,752 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:45,752 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:45,754 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:45,754 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:45,754 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:45,758 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:45,758 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:45,758 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:45,758 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:45,762 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:45,762 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:45,762 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:45,763 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.input_layernorm.weight, converting to float16
2025-08-04 18:41:45,763 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:45,764 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:45,765 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:45,777 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:45,778 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:45,778 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:45,791 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:45,791 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:45,791 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:45,804 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:45,804 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:45,804 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:45,804 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:45,804 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:45,806 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:45,806 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:45,806 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:45,810 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:45,810 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:45,810 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:45,810 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:45,814 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:45,814 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:45,814 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:45,816 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.input_layernorm.weight, converting to float16
2025-08-04 18:41:45,816 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:45,817 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:45,817 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:45,829 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:45,830 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:45,830 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:45,843 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:45,844 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:45,845 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:45,857 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:45,857 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:45,857 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:45,857 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:45,857 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:45,859 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:45,859 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:45,859 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:45,862 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:45,862 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:45,862 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:45,862 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:45,866 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:45,866 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:45,866 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:45,868 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.input_layernorm.weight, converting to float16
2025-08-04 18:41:45,868 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:45,869 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:45,869 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:45,881 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:45,881 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:45,882 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:45,896 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:45,896 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:45,897 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:45,905 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:45,905 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:45,905 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:45,905 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:45,905 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:45,907 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:45,907 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:45,907 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:45,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:45,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:45,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:45,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:45,916 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:45,916 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:45,916 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:45,918 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.input_layernorm.weight, converting to float16
2025-08-04 18:41:45,918 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:45,918 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:45,919 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:45,932 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:45,933 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:45,933 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:45,946 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:45,946 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:45,947 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:45,957 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:45,957 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:45,957 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:45,958 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:45,958 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:45,960 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:45,960 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:45,960 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:45,964 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:45,964 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:45,964 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:45,964 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:45,967 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:45,967 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:45,967 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:45,969 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.input_layernorm.weight, converting to float16
2025-08-04 18:41:45,969 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:45,970 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:45,970 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:45,979 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:45,980 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:45,980 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:45,994 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:45,995 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:45,995 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,007 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,007 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,007 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,007 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,007 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,009 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,009 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,010 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,014 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,014 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,014 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,014 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,018 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,018 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,018 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,020 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,020 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,020 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,021 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,034 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,034 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,035 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,048 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,048 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,049 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,062 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,062 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,062 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,066 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,066 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,066 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,067 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,071 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,071 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,071 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,073 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,073 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,073 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,074 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,086 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,087 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,087 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,100 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,100 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,101 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,115 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,115 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,115 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,119 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,119 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,119 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,119 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,123 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,123 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,123 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,125 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,125 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,126 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,126 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,139 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,139 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,140 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,152 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,153 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,154 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,166 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,166 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,166 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,166 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,166 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,168 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,168 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,168 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,173 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,173 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,173 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,173 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,177 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,177 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,177 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,179 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,179 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,179 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,180 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,192 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,193 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,193 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,206 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,207 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,208 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,220 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,220 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,220 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,220 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,220 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,222 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,222 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,222 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,225 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,225 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,225 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,225 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,229 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,229 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,229 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,231 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,231 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,232 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,232 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,245 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,245 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,245 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,254 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,254 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,255 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,264 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,264 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,265 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,265 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,265 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,267 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,267 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,267 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,271 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,271 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,271 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,271 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,275 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,275 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,275 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,277 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,277 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,277 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,278 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,290 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,290 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,291 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,303 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,304 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,304 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,316 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,316 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,317 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,317 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,317 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,319 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,319 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,319 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,323 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,323 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,323 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,323 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,327 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,327 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,327 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,329 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,329 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,329 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,330 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,342 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,355 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,356 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,356 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,368 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,368 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,368 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,368 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,368 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,370 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,370 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,370 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,374 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,374 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,374 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,374 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,378 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,378 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,378 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,380 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,380 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,381 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,381 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,393 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,394 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,395 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,407 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,407 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,408 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,417 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,417 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,417 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,417 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,417 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,419 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,419 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,419 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,422 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,422 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,422 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,422 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,426 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,426 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,426 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,428 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,428 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,428 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,429 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,442 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,442 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,443 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,455 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,456 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,456 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,465 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,465 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,465 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,465 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,465 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,467 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,467 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,467 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,471 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,471 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,471 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,471 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,475 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,475 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,475 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,477 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,477 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,478 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,478 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,491 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,491 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,491 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,504 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,504 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,505 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,517 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,517 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,517 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,517 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,517 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,519 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,519 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,519 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,523 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,523 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,523 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,523 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,527 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,527 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,527 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,529 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,529 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,530 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,530 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,543 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,544 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,544 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,556 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,556 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,557 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,569 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,569 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,569 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,569 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,569 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,571 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,571 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,571 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,575 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,575 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,575 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,575 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,579 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,579 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,580 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,581 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,581 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,582 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,582 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,594 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,595 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,595 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,622 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,622 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,622 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,622 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,622 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,624 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,624 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,624 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,628 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,628 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,628 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,628 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,632 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,632 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,632 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,634 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,634 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,634 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,634 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,647 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,647 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,648 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,660 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,661 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,661 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,670 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,670 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,670 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,670 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,670 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,676 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,676 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,676 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,676 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,681 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,681 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,681 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,683 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,683 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,683 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,684 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,696 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,697 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,697 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,709 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,710 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,711 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,724 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,724 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,724 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,724 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,724 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,726 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,726 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,726 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,730 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,730 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,730 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,730 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,734 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,734 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,734 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,736 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,736 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,737 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,737 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,749 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,750 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,751 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,763 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,764 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,765 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,777 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,777 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,777 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,777 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,777 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,779 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,779 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,779 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,784 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,784 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,784 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,784 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,788 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,788 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,788 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,789 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,789 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,790 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,790 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,802 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,803 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,804 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,816 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,817 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,817 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,830 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,830 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,830 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,830 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,830 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,833 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,837 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,837 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,837 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,837 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,841 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,841 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,841 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,843 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,843 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,843 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,844 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,854 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,855 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,855 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,866 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,867 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,867 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,880 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,880 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,880 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,880 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,880 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,882 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,882 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,882 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,886 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,886 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,886 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,886 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,889 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,889 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,889 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,891 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.input_layernorm.weight, converting to float16
2025-08-04 18:41:46,891 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.down_proj.biases, converting to float16
2025-08-04 18:41:46,891 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.down_proj.scales, converting to float16
2025-08-04 18:41:46,892 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.down_proj.weight, converting to float16
2025-08-04 18:41:46,900 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.gate_proj.biases, converting to float16
2025-08-04 18:41:46,901 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.gate_proj.scales, converting to float16
2025-08-04 18:41:46,901 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.gate_proj.weight, converting to float16
2025-08-04 18:41:46,914 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.up_proj.biases, converting to float16
2025-08-04 18:41:46,915 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.up_proj.scales, converting to float16
2025-08-04 18:41:46,915 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.up_proj.weight, converting to float16
2025-08-04 18:41:46,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.post_attention_layernorm.weight, converting to float16
2025-08-04 18:41:46,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_norm.weight, converting to float16
2025-08-04 18:41:46,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_proj.biases, converting to float16
2025-08-04 18:41:46,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_proj.scales, converting to float16
2025-08-04 18:41:46,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.k_proj.weight, converting to float16
2025-08-04 18:41:46,930 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.o_proj.biases, converting to float16
2025-08-04 18:41:46,930 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.o_proj.scales, converting to float16
2025-08-04 18:41:46,930 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.o_proj.weight, converting to float16
2025-08-04 18:41:46,934 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_norm.weight, converting to float16
2025-08-04 18:41:46,934 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_proj.biases, converting to float16
2025-08-04 18:41:46,934 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_proj.scales, converting to float16
2025-08-04 18:41:46,934 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.q_proj.weight, converting to float16
2025-08-04 18:41:46,938 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.v_proj.biases, converting to float16
2025-08-04 18:41:46,938 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.v_proj.scales, converting to float16
2025-08-04 18:41:46,938 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.v_proj.weight, converting to float16
2025-08-04 18:41:46,940 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.norm.weight, converting to float16
2025-08-04 18:41:46,940 - [src.utils.mlx_pytorch_adapter] - INFO - Converted 704 weights
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,909 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,910 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,911 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,912 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:42:05,913 - [src.utils.mlx_pytorch_adapter] - INFO - Loaded 113 weights, missing 198
2025-08-04 18:42:07,513 - [__main__] - INFO - Successfully loaded model with 28 assigned layers
2025-08-04 18:42:07,513 - [__main__] - INFO - Initializing KV cache...
2025-08-04 18:42:07,513 - [src.core.pytorch_kv_cache] - INFO - Device mini1: 716 sequences, 11468.8MB budget, 100.0% compute share
2025-08-04 18:42:07,513 - [__main__] - INFO - Initialized KV cache with size 716 for 28 layers
2025-08-04 18:42:07,513 - [__main__] - INFO - Setting up FastAPI server...
2025-08-04 18:42:07,629 - [__main__] - INFO - Server initialization complete
2025-08-04 18:42:07,647 - [__main__] - INFO - Starting API server on 0.0.0.0:8100
INFO:     Started server process [4881]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8100 (Press CTRL+C to quit)
INFO:     192.168.2.103:50179 - "GET /docs HTTP/1.1" 200 OK
INFO:     192.168.2.103:50179 - "GET /openapi.json HTTP/1.1" 200 OK
INFO:     192.168.2.103:50179 - "GET /health HTTP/1.1" 200 OK
2025-08-04 18:43:06,371 - [__main__] - INFO - Starting distributed generation: 'hi mom!...'
INFO:     192.168.2.103:50180 - "POST /generate HTTP/1.1" 200 OK
INFO:     127.0.0.1:53546 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:53579 - "GET /cache/stats HTTP/1.1" 500 Internal Server Error
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/uvicorn/protocols/http/httptools_impl.py", line 409, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        self.scope, self.receive, self.send
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/fastapi/applications.py", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/applications.py", line 113, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/routing.py", line 78, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/routing.py", line 75, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/fastapi/routing.py", line 302, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
    )
    ^
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/fastapi/routing.py", line 213, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/mini1/Movies/mlx_grpc_inference/server_file_based.py", line 243, in cache_stats
    "cache_size": self.kv_cache.cache_size,
                  ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DistributedKVCacheManager' object has no attribute 'cache_size'
INFO:     127.0.0.1:53605 - "GET /cache/stats HTTP/1.1" 500 Internal Server Error
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/uvicorn/protocols/http/httptools_impl.py", line 409, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        self.scope, self.receive, self.send
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/fastapi/applications.py", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/applications.py", line 113, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/routing.py", line 78, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/starlette/routing.py", line 75, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/fastapi/routing.py", line 302, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
    )
    ^
  File "/Users/mini1/Movies/mlx_grpc_inference/.venv/lib/python3.13/site-packages/fastapi/routing.py", line 213, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/mini1/Movies/mlx_grpc_inference/server_file_based.py", line 243, in cache_stats
    "cache_size": self.kv_cache.cache_size,
                  ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DistributedKVCacheManager' object has no attribute 'cache_size'
