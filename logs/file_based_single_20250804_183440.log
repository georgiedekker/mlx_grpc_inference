2025-08-04 18:34:43,090 - [__main__] - INFO - Parsed environment: RANK=0, WORLD_SIZE=1
2025-08-04 18:34:43,103 - [src.coordination.file_based_coordinator] - INFO - Detected Thunderbolt IP: 192.168.5.1
2025-08-04 18:34:43,104 - [src.coordination.file_based_coordinator] - INFO - Initialized file-based coordinator: rank=0, world_size=1
2025-08-04 18:34:43,104 - [__main__] - INFO - Initialized file-based server: rank=0, world_size=1
2025-08-04 18:34:43,104 - [__main__] - INFO - Initializing file-based coordination...
2025-08-04 18:34:43,104 - [src.coordination.file_based_coordinator] - INFO - Waiting for 1 nodes to register...
2025-08-04 18:34:43,104 - [src.coordination.file_based_coordinator] - INFO - All nodes registered and alive
2025-08-04 18:34:43,107 - [src.coordination.file_based_coordinator] - INFO - Assigned layers: {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]}
2025-08-04 18:34:43,107 - [src.coordination.file_based_coordinator] - INFO - Loading layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
2025-08-04 18:34:45,110 - [src.coordination.file_based_coordinator] - INFO - Loaded 28 layers
2025-08-04 18:34:45,111 - [src.coordination.file_based_coordinator] - INFO - Distributed inference system initialized successfully
2025-08-04 18:34:45,111 - [__main__] - INFO - Loading device capabilities...
2025-08-04 18:34:45,115 - [__main__] - INFO - Loading assigned model layers...
2025-08-04 18:34:45,115 - [__main__] - INFO - Loading layers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
2025-08-04 18:34:45,120 - [__main__] - INFO - Loading MLX model: mlx-community/Qwen3-1.7B-8bit
2025-08-04 18:34:45,120 - [__main__] - INFO - Using device: mps
2025-08-04 18:34:45,120 - [src.utils.mlx_pytorch_adapter] - INFO - Loading MLX model mlx-community/Qwen3-1.7B-8bit for PyTorch
2025-08-04 18:34:45,121 - [src.utils.mlx_pytorch_adapter] - INFO - Converting MLX model from /Users/mini1/.cache/huggingface/hub/models--mlx-community--Qwen3-1.7B-8bit/snapshots/8c24f6782a91421513803ce527a27dcc560ab904
2025-08-04 18:34:45,126 - [src.utils.mlx_pytorch_adapter] - INFO - Found weight files: ['model.safetensors']
2025-08-04 18:34:45,278 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.embed_tokens.biases, converting to float16
2025-08-04 18:34:45,313 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.embed_tokens.scales, converting to float16
2025-08-04 18:34:45,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.embed_tokens.weight, converting to float16
2025-08-04 18:34:46,181 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.input_layernorm.weight, converting to float16
2025-08-04 18:34:46,181 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:46,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:46,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:46,203 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:46,205 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:46,205 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:46,225 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:46,226 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:46,226 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:46,245 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:46,245 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:46,245 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:46,245 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:46,245 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:46,249 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:46,250 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:46,250 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:46,259 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:46,259 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:46,259 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:46,260 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:46,266 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:46,266 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:46,266 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:46,269 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.input_layernorm.weight, converting to float16
2025-08-04 18:34:46,269 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:46,270 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:46,271 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:46,297 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:46,297 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:46,298 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:46,318 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:46,319 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:46,319 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:46,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:46,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:46,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:46,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:46,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:46,347 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:46,348 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:46,348 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:46,355 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:46,355 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:46,356 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:46,357 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:46,375 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:46,376 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:46,377 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:46,389 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.input_layernorm.weight, converting to float16
2025-08-04 18:34:46,389 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:46,392 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:46,393 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:46,431 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:46,432 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:46,433 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:46,456 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:46,457 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:46,458 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:46,477 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:46,478 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:46,478 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:46,478 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:46,478 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:46,484 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:46,484 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:46,484 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:46,491 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:46,491 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:46,491 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:46,492 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:46,499 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:46,500 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:46,501 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:46,504 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.input_layernorm.weight, converting to float16
2025-08-04 18:34:46,504 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:46,505 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:46,506 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:46,529 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:46,530 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:46,531 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:46,555 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:46,556 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:46,558 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:46,575 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:46,575 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:46,575 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:46,575 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:46,576 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:46,579 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:46,579 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:46,579 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:46,584 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:46,584 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:46,584 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:46,585 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:46,593 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:46,593 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:46,593 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:46,597 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.input_layernorm.weight, converting to float16
2025-08-04 18:34:46,597 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:46,598 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:46,599 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:46,619 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:46,620 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:46,622 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:46,642 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:46,643 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:46,644 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:46,665 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:46,665 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:46,665 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:46,665 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:46,665 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:46,667 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:46,668 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:46,668 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:46,675 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:46,675 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:46,676 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:46,676 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:46,685 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:46,685 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:46,686 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:46,689 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.input_layernorm.weight, converting to float16
2025-08-04 18:34:46,689 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:46,689 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:46,690 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:46,710 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:46,711 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:46,712 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:46,734 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:46,735 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:46,736 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:46,757 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:46,757 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:46,757 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:46,757 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:46,757 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:46,760 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:46,760 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:46,760 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:46,766 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:46,766 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:46,767 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:46,767 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:46,774 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:46,774 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:46,774 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:46,778 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.input_layernorm.weight, converting to float16
2025-08-04 18:34:46,778 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:46,779 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:46,779 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:46,798 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:46,799 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:46,800 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:46,819 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:46,820 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:46,820 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:46,842 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:46,843 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:46,843 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:46,843 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:46,843 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:46,848 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:46,849 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:46,850 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:46,856 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:46,856 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:46,857 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:46,857 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:46,865 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:46,865 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:46,865 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:46,869 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.input_layernorm.weight, converting to float16
2025-08-04 18:34:46,869 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:46,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:46,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:46,893 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:46,894 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:46,894 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:46,913 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:46,914 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:46,915 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:46,934 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:46,934 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:46,934 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:46,934 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:46,934 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:46,938 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:46,939 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:46,939 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:46,944 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:46,945 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:46,945 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:46,946 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:46,950 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:46,950 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:46,950 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:46,953 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.input_layernorm.weight, converting to float16
2025-08-04 18:34:46,953 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:46,953 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:46,954 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:46,972 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:46,973 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:46,974 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:46,996 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:46,996 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:46,997 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:47,022 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:47,022 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:47,022 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:47,022 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:47,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:47,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:47,029 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:47,030 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:47,037 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:47,037 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:47,037 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:47,038 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:47,046 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:47,046 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:47,047 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:47,051 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.input_layernorm.weight, converting to float16
2025-08-04 18:34:47,051 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:47,052 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:47,054 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:47,072 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:47,073 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:47,074 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:47,092 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:47,093 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:47,093 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:47,117 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:47,117 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:47,117 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:47,117 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:47,117 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:47,121 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:47,122 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:47,122 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:47,130 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:47,130 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:47,130 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:47,131 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:47,138 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:47,138 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:47,138 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:47,143 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.input_layernorm.weight, converting to float16
2025-08-04 18:34:47,143 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:47,143 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:47,144 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:47,164 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:47,164 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:47,166 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:47,187 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:47,188 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:47,189 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:47,209 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:47,209 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:47,209 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:47,209 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:47,209 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:47,213 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:47,213 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:47,214 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:47,222 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:47,222 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:47,223 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:47,223 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:47,231 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:47,231 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:47,231 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:47,235 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.input_layernorm.weight, converting to float16
2025-08-04 18:34:47,235 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:47,236 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:47,237 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:47,258 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:47,259 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:47,260 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:47,279 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:47,280 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:47,281 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:47,300 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:47,300 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:47,300 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:47,300 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:47,300 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:47,302 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:47,302 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:47,302 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:47,311 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:47,311 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:47,311 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:47,312 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:47,321 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:47,322 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:47,322 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:47,327 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.input_layernorm.weight, converting to float16
2025-08-04 18:34:47,327 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:47,328 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:47,328 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:47,364 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:47,365 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:47,366 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:47,388 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:47,389 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:47,389 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:47,412 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:47,412 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:47,412 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:47,413 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:47,413 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:47,415 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:47,416 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:47,416 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:47,424 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:47,424 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:47,424 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:47,425 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:47,433 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:47,433 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:47,433 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:47,437 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.input_layernorm.weight, converting to float16
2025-08-04 18:34:47,437 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:47,438 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:47,439 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:47,458 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:47,460 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:47,461 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:47,483 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:47,483 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:47,484 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:47,512 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:47,512 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:47,512 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:47,512 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:47,512 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:47,518 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:47,518 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:47,518 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:47,522 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:47,522 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:47,523 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:47,523 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:47,534 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:47,535 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:47,535 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:47,539 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.input_layernorm.weight, converting to float16
2025-08-04 18:34:47,539 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:47,540 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:47,541 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:47,563 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:47,564 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:47,565 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:47,585 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:47,587 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:47,587 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:47,603 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:47,603 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:47,603 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:47,603 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:47,604 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:47,607 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:47,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:47,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:47,619 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:47,619 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:47,620 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:47,621 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:47,627 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:47,628 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:47,628 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:47,632 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.input_layernorm.weight, converting to float16
2025-08-04 18:34:47,632 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:47,633 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:47,634 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:47,656 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:47,657 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:47,657 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:47,679 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:47,680 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:47,681 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:47,699 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:47,699 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:47,699 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:47,699 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:47,699 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:47,704 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:47,704 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:47,704 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:47,709 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:47,709 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:47,709 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:47,709 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:47,716 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:47,716 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:47,717 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:47,721 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.input_layernorm.weight, converting to float16
2025-08-04 18:34:47,721 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:47,722 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:47,722 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:47,742 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:47,743 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:47,744 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:47,766 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:47,767 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:47,768 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:47,790 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:47,790 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:47,790 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:47,790 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:47,790 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:47,792 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:47,793 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:47,793 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:47,798 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:47,798 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:47,798 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:47,799 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:47,807 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:47,807 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:47,807 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:47,812 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.input_layernorm.weight, converting to float16
2025-08-04 18:34:47,812 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:47,813 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:47,814 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:47,835 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:47,836 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:47,837 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:47,863 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:47,863 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:47,865 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:47,886 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:47,886 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:47,886 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:47,886 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:47,886 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:47,888 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:47,888 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:47,889 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:47,892 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:47,892 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:47,892 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:47,893 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:47,900 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:47,900 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:47,900 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:47,905 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.input_layernorm.weight, converting to float16
2025-08-04 18:34:47,905 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:47,906 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:47,907 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:47,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:47,930 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:47,930 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:47,951 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:47,952 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:47,953 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:47,972 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:47,972 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:47,973 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:47,973 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:47,973 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:47,978 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:47,979 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:47,979 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:47,986 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:47,986 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:47,986 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:47,987 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:47,996 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:47,996 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:47,996 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:47,998 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.input_layernorm.weight, converting to float16
2025-08-04 18:34:47,998 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:47,999 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:48,000 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:48,020 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:48,020 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:48,021 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:48,041 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:48,042 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:48,043 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:48,068 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:48,068 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:48,068 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:48,068 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:48,069 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:48,073 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:48,073 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:48,073 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:48,082 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:48,082 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:48,082 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:48,083 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:48,088 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:48,089 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:48,089 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:48,093 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.input_layernorm.weight, converting to float16
2025-08-04 18:34:48,093 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:48,094 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:48,095 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:48,113 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:48,114 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:48,115 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:48,135 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:48,136 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:48,136 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:48,156 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:48,156 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:48,156 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:48,157 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:48,157 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:48,166 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:48,166 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:48,166 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:48,174 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:48,174 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:48,174 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:48,175 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:48,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:48,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:48,184 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:48,188 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.input_layernorm.weight, converting to float16
2025-08-04 18:34:48,188 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:48,188 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:48,189 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:48,209 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:48,210 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:48,211 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:48,232 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:48,233 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:48,234 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:48,256 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:48,256 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:48,256 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:48,256 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:48,256 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:48,260 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:48,260 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:48,261 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:48,268 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:48,268 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:48,268 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:48,269 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:48,278 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:48,278 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:48,279 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:48,282 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.input_layernorm.weight, converting to float16
2025-08-04 18:34:48,282 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:48,283 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:48,284 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:48,306 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:48,307 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:48,307 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:48,326 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:48,327 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:48,327 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:48,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:48,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:48,343 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:48,344 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:48,344 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:48,348 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:48,349 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:48,349 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:48,357 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:48,357 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:48,358 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:48,359 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:48,365 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:48,365 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:48,365 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:48,370 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.input_layernorm.weight, converting to float16
2025-08-04 18:34:48,370 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:48,371 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:48,372 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:48,390 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:48,391 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:48,392 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:48,408 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:48,410 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:48,411 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:48,432 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:48,433 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:48,433 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:48,433 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:48,433 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:48,436 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:48,437 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:48,438 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:48,446 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:48,446 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:48,446 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:48,446 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:48,452 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:48,452 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:48,453 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:48,457 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.input_layernorm.weight, converting to float16
2025-08-04 18:34:48,457 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:48,458 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:48,459 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:48,479 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:48,480 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:48,480 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:48,503 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:48,504 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:48,505 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:48,525 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:48,525 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:48,525 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:48,525 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:48,526 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:48,531 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:48,532 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:48,532 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:48,543 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:48,543 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:48,544 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:48,545 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:48,556 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:48,556 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:48,557 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:48,570 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.input_layernorm.weight, converting to float16
2025-08-04 18:34:48,571 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:48,574 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:48,576 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:48,599 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:48,600 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:48,602 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:48,625 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:48,626 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:48,627 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:48,647 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:48,647 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:48,647 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:48,647 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:48,648 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:48,652 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:48,653 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:48,653 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:48,660 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:48,660 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:48,661 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:48,661 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:48,666 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:48,666 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:48,666 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:48,671 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.input_layernorm.weight, converting to float16
2025-08-04 18:34:48,671 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:48,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:48,673 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:48,696 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:48,696 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:48,697 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:48,718 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:48,719 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:48,720 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:48,740 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:48,740 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:48,740 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:48,740 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:48,741 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:48,743 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:48,743 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:48,743 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:48,751 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:48,751 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:48,752 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:48,754 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:48,758 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:48,758 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:48,759 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:48,761 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.input_layernorm.weight, converting to float16
2025-08-04 18:34:48,762 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.down_proj.biases, converting to float16
2025-08-04 18:34:48,762 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.down_proj.scales, converting to float16
2025-08-04 18:34:48,763 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.down_proj.weight, converting to float16
2025-08-04 18:34:48,785 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.gate_proj.biases, converting to float16
2025-08-04 18:34:48,785 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.gate_proj.scales, converting to float16
2025-08-04 18:34:48,786 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.gate_proj.weight, converting to float16
2025-08-04 18:34:48,809 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.up_proj.biases, converting to float16
2025-08-04 18:34:48,810 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.up_proj.scales, converting to float16
2025-08-04 18:34:48,810 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.up_proj.weight, converting to float16
2025-08-04 18:34:48,844 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.post_attention_layernorm.weight, converting to float16
2025-08-04 18:34:48,844 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_norm.weight, converting to float16
2025-08-04 18:34:48,844 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_proj.biases, converting to float16
2025-08-04 18:34:48,845 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_proj.scales, converting to float16
2025-08-04 18:34:48,845 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.k_proj.weight, converting to float16
2025-08-04 18:34:48,848 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.o_proj.biases, converting to float16
2025-08-04 18:34:48,849 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.o_proj.scales, converting to float16
2025-08-04 18:34:48,849 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.o_proj.weight, converting to float16
2025-08-04 18:34:48,868 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_norm.weight, converting to float16
2025-08-04 18:34:48,869 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_proj.biases, converting to float16
2025-08-04 18:34:48,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_proj.scales, converting to float16
2025-08-04 18:34:48,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.q_proj.weight, converting to float16
2025-08-04 18:34:48,892 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.v_proj.biases, converting to float16
2025-08-04 18:34:48,893 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.v_proj.scales, converting to float16
2025-08-04 18:34:48,893 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.v_proj.weight, converting to float16
2025-08-04 18:34:48,901 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.norm.weight, converting to float16
2025-08-04 18:34:48,901 - [src.utils.mlx_pytorch_adapter] - INFO - Converted 704 weights
2025-08-04 18:35:10,607 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,608 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:35:10,612 - [src.utils.mlx_pytorch_adapter] - INFO - Loaded 113 weights, missing 198
2025-08-04 18:35:12,629 - [__main__] - INFO - Successfully loaded model with 28 assigned layers
2025-08-04 18:35:12,630 - [__main__] - INFO - Initializing KV cache...
2025-08-04 18:35:12,631 - [__main__] - ERROR - Failed to initialize KV cache: 'HeterogeneousCacheAllocator' object has no attribute 'allocate_cache_memory'
2025-08-04 18:35:12,631 - [__main__] - ERROR - Server initialization failed: 'HeterogeneousCacheAllocator' object has no attribute 'allocate_cache_memory'
2025-08-04 18:35:12,631 - [__main__] - ERROR - Server initialization failed
2025-08-04 18:35:12,631 - [__main__] - INFO - Shutting down server...
2025-08-04 18:35:12,632 - [src.coordination.file_based_coordinator] - INFO - Cleaned up coordination files
