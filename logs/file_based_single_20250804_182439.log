2025-08-04 18:24:40,838 - [__main__] - INFO - Parsed environment: RANK=0, WORLD_SIZE=1
2025-08-04 18:24:40,845 - [src.coordination.file_based_coordinator] - INFO - Detected Thunderbolt IP: 192.168.5.1
2025-08-04 18:24:40,845 - [src.coordination.file_based_coordinator] - INFO - Initialized file-based coordinator: rank=0, world_size=1
2025-08-04 18:24:40,845 - [__main__] - INFO - Initialized file-based server: rank=0, world_size=1
2025-08-04 18:24:40,845 - [__main__] - INFO - Initializing file-based coordination...
2025-08-04 18:24:40,846 - [src.coordination.file_based_coordinator] - INFO - Waiting for 1 nodes to register...
2025-08-04 18:24:40,846 - [src.coordination.file_based_coordinator] - INFO - All nodes registered and alive
2025-08-04 18:24:40,848 - [src.coordination.file_based_coordinator] - INFO - Assigned layers: {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]}
2025-08-04 18:24:40,848 - [src.coordination.file_based_coordinator] - INFO - Loading layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
2025-08-04 18:24:42,858 - [src.coordination.file_based_coordinator] - INFO - Loaded 28 layers
2025-08-04 18:24:42,860 - [src.coordination.file_based_coordinator] - INFO - Distributed inference system initialized successfully
2025-08-04 18:24:42,860 - [__main__] - INFO - Loading device capabilities...
2025-08-04 18:24:42,870 - [__main__] - INFO - Loading assigned model layers...
2025-08-04 18:24:42,870 - [__main__] - INFO - Loading layers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
2025-08-04 18:24:42,876 - [__main__] - INFO - Loading MLX model: mlx-community/Qwen3-1.7B-8bit
2025-08-04 18:24:42,877 - [__main__] - INFO - Using device: mps
2025-08-04 18:24:42,877 - [src.utils.mlx_pytorch_adapter] - INFO - Loading MLX model mlx-community/Qwen3-1.7B-8bit for PyTorch
2025-08-04 18:24:42,878 - [src.utils.mlx_pytorch_adapter] - INFO - Converting MLX model from /Users/mini1/.cache/huggingface/hub/models--mlx-community--Qwen3-1.7B-8bit/snapshots/8c24f6782a91421513803ce527a27dcc560ab904
2025-08-04 18:24:42,885 - [src.utils.mlx_pytorch_adapter] - INFO - Found weight files: ['model.safetensors']
2025-08-04 18:24:42,981 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.embed_tokens.biases, converting to float16
2025-08-04 18:24:42,999 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.embed_tokens.scales, converting to float16
2025-08-04 18:24:43,017 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.embed_tokens.weight, converting to float16
2025-08-04 18:24:43,500 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.input_layernorm.weight, converting to float16
2025-08-04 18:24:43,500 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:43,502 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:43,503 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:43,522 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:43,523 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:43,524 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:43,542 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:43,543 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:43,544 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:43,562 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:43,562 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:43,562 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:43,562 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:43,562 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:43,568 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:43,569 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:43,569 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:43,579 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:43,579 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:43,579 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:43,580 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:43,589 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:43,589 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.0.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:43,590 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.0.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:43,595 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.input_layernorm.weight, converting to float16
2025-08-04 18:24:43,595 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:43,596 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:43,597 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:43,623 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:43,624 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:43,625 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:43,645 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:43,647 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:43,648 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:43,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:43,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:43,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:43,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:43,672 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:43,677 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:43,678 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:43,678 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:43,687 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:43,687 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:43,688 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:43,689 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:43,698 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:43,698 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.1.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:43,698 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.1.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:43,703 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.input_layernorm.weight, converting to float16
2025-08-04 18:24:43,703 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:43,703 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:43,704 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:43,731 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:43,732 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:43,733 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:43,755 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:43,756 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:43,757 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:43,783 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:43,783 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:43,783 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:43,783 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:43,783 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:43,787 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:43,787 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:43,787 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:43,794 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:43,794 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:43,794 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:43,794 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:43,798 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:43,798 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.10.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:43,798 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.10.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:43,801 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.input_layernorm.weight, converting to float16
2025-08-04 18:24:43,801 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:43,802 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:43,803 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:43,826 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:43,827 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:43,827 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:43,847 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:43,848 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:43,849 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:43,869 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:43,869 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:43,869 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:43,870 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:43,871 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:43,875 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:43,876 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:43,876 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:43,882 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:43,882 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:43,882 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:43,882 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:43,891 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:43,891 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.11.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:43,891 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.11.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:43,895 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.input_layernorm.weight, converting to float16
2025-08-04 18:24:43,895 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:43,896 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:43,897 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:43,921 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:43,922 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:43,923 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:43,948 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:43,949 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:43,950 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:43,971 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:43,971 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:43,971 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:43,972 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:43,972 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:43,976 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:43,977 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:43,977 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:43,985 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:43,985 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:43,985 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:43,986 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:43,993 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:43,993 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.12.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:43,994 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.12.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:43,998 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.input_layernorm.weight, converting to float16
2025-08-04 18:24:43,998 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:43,999 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:44,000 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:44,018 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:44,019 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:44,020 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:44,039 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:44,039 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:44,040 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:44,063 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:44,063 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:44,063 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:44,063 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:44,063 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:44,068 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:44,068 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:44,068 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:44,075 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:44,075 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:44,075 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:44,075 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:44,081 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:44,081 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.13.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:44,081 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.13.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:44,086 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.input_layernorm.weight, converting to float16
2025-08-04 18:24:44,086 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:44,087 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:44,088 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:44,110 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:44,111 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:44,111 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:44,130 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:44,131 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:44,132 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:44,150 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:44,150 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:44,150 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:44,150 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:44,150 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:44,154 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:44,155 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:44,156 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:44,161 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:44,162 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:44,162 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:44,162 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:44,170 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:44,170 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.14.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:44,171 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.14.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:44,175 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.input_layernorm.weight, converting to float16
2025-08-04 18:24:44,175 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:44,175 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:44,176 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:44,194 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:44,195 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:44,196 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:44,215 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:44,217 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:44,217 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:44,234 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:44,235 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:44,235 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:44,235 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:44,235 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:44,237 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:44,237 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:44,238 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:44,244 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:44,244 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:44,245 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:44,245 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:44,253 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:44,254 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.15.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:44,254 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.15.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:44,258 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.input_layernorm.weight, converting to float16
2025-08-04 18:24:44,258 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:44,259 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:44,259 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:44,286 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:44,286 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:44,287 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:44,313 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:44,314 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:44,314 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:44,338 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:44,338 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:44,338 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:44,339 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:44,339 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:44,344 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:44,345 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:44,346 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:44,352 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:44,352 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:44,352 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:44,353 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:44,363 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:44,363 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.16.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:44,363 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.16.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:44,365 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.input_layernorm.weight, converting to float16
2025-08-04 18:24:44,365 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:44,365 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:44,366 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:44,385 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:44,386 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:44,387 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:44,405 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:44,406 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:44,407 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:44,431 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:44,431 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:44,431 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:44,431 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:44,431 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:44,434 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:44,434 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:44,435 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:44,445 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:44,445 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:44,445 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:44,445 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:44,452 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:44,452 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.17.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:44,452 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.17.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:44,457 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.input_layernorm.weight, converting to float16
2025-08-04 18:24:44,457 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:44,457 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:44,458 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:44,482 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:44,483 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:44,484 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:44,504 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:44,505 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:44,506 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:44,535 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:44,535 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:44,535 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:44,535 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:44,535 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:44,538 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:44,539 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:44,540 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:44,550 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:44,550 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:44,550 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:44,551 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:44,558 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:44,559 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.18.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:44,559 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.18.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:44,566 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.input_layernorm.weight, converting to float16
2025-08-04 18:24:44,566 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:44,568 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:44,568 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:44,588 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:44,589 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:44,590 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:44,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:44,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:44,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:44,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:44,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:44,630 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:44,631 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:44,631 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:44,633 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:44,634 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:44,634 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:44,640 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:44,640 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:44,640 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:44,641 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:44,647 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:44,648 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.19.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:44,648 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.19.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:44,651 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.input_layernorm.weight, converting to float16
2025-08-04 18:24:44,651 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:44,652 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:44,653 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:44,673 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:44,674 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:44,675 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:44,697 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:44,698 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:44,699 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:44,722 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:44,722 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:44,722 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:44,722 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:44,722 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:44,727 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:44,728 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:44,728 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:44,736 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:44,736 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:44,736 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:44,736 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:44,743 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:44,744 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.2.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:44,744 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.2.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:44,746 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.input_layernorm.weight, converting to float16
2025-08-04 18:24:44,746 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:44,747 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:44,748 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:44,772 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:44,773 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:44,774 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:44,806 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:44,807 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:44,808 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:44,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:44,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:44,832 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:44,833 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:44,833 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:44,837 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:44,838 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:44,838 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:44,847 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:44,847 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:44,848 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:44,848 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:44,854 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:44,854 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.20.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:44,854 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.20.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:44,858 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.input_layernorm.weight, converting to float16
2025-08-04 18:24:44,858 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:44,859 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:44,860 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:44,882 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:44,883 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:44,884 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:44,906 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:44,906 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:44,907 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:44,923 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:44,923 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:44,923 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:44,923 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:44,923 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:44,928 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:44,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:44,929 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:44,939 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:44,939 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:44,940 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:44,941 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:44,948 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:44,948 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.21.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:44,948 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.21.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:44,956 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.input_layernorm.weight, converting to float16
2025-08-04 18:24:44,956 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:44,956 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:44,957 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:44,980 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:44,981 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:44,982 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:45,006 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:45,007 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:45,008 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:45,037 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:45,037 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:45,037 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:45,037 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:45,038 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:45,042 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:45,043 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:45,043 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:45,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:45,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:45,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:45,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:45,067 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:45,067 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.22.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:45,067 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.22.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:45,071 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.input_layernorm.weight, converting to float16
2025-08-04 18:24:45,071 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:45,074 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:45,076 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:45,103 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:45,104 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:45,105 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:45,123 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:45,124 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:45,125 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:45,147 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:45,147 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:45,147 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:45,147 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:45,147 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:45,153 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:45,154 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:45,154 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:45,163 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:45,163 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:45,164 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:45,165 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:45,176 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:45,176 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.23.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:45,176 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.23.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:45,181 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.input_layernorm.weight, converting to float16
2025-08-04 18:24:45,181 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:45,182 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:45,183 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:45,208 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:45,210 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:45,211 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:45,238 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:45,239 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:45,240 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:45,270 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:45,270 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:45,270 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:45,270 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:45,271 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:45,273 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:45,273 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:45,273 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:45,285 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:45,285 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:45,286 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:45,286 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:45,291 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:45,292 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.24.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:45,292 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.24.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:45,296 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.input_layernorm.weight, converting to float16
2025-08-04 18:24:45,296 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:45,297 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:45,298 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:45,331 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:45,332 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:45,333 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:45,358 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:45,358 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:45,359 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:45,381 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:45,381 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:45,381 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:45,381 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:45,381 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:45,383 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:45,384 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:45,384 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:45,388 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:45,388 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:45,389 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:45,390 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:45,400 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:45,401 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.25.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:45,401 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.25.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:45,403 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.input_layernorm.weight, converting to float16
2025-08-04 18:24:45,403 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:45,404 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:45,405 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:45,430 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:45,431 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:45,431 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:45,455 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:45,457 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:45,457 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:45,478 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:45,478 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:45,478 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:45,478 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:45,478 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:45,482 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:45,482 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:45,482 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:45,493 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:45,493 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:45,493 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:45,493 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:45,506 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:45,506 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.26.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:45,506 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.26.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:45,509 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.input_layernorm.weight, converting to float16
2025-08-04 18:24:45,509 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:45,510 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:45,515 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:45,538 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:45,542 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:45,542 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:45,556 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:45,557 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:45,558 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:45,587 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:45,587 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:45,587 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:45,587 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:45,587 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:45,589 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:45,590 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:45,590 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:45,596 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:45,596 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:45,597 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:45,598 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:45,606 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:45,607 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.27.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:45,607 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.27.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:45,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.input_layernorm.weight, converting to float16
2025-08-04 18:24:45,609 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:45,610 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:45,611 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:45,636 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:45,636 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:45,637 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:45,662 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:45,663 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:45,663 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:45,687 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:45,687 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:45,687 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:45,688 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:45,688 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:45,691 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:45,692 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:45,692 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:45,702 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:45,702 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:45,702 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:45,702 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:45,706 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:45,706 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.3.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:45,706 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.3.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:45,710 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.input_layernorm.weight, converting to float16
2025-08-04 18:24:45,710 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:45,711 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:45,712 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:45,748 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:45,749 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:45,750 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:45,781 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:45,781 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:45,782 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:45,802 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:45,802 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:45,802 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:45,802 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:45,803 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:45,809 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:45,810 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:45,810 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:45,830 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:45,830 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:45,830 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:45,830 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:45,835 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:45,835 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.4.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:45,835 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.4.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:45,839 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.input_layernorm.weight, converting to float16
2025-08-04 18:24:45,839 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:45,840 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:45,841 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:45,873 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:45,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:45,874 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:45,898 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:45,899 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:45,899 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:45,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:45,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:45,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:45,925 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:45,926 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:45,930 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:45,930 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:45,931 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:45,942 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:45,942 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:45,943 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:45,943 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:45,949 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:45,949 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.5.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:45,950 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.5.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:45,959 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.input_layernorm.weight, converting to float16
2025-08-04 18:24:45,959 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:45,960 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:45,961 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:45,984 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:45,985 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:45,986 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:46,016 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:46,017 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:46,017 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:46,043 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:46,043 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:46,043 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:46,043 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:46,043 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:46,046 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:46,047 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:46,048 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:46,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:46,060 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:46,061 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:46,062 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:46,071 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:46,071 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.6.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:46,071 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.6.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:46,074 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.input_layernorm.weight, converting to float16
2025-08-04 18:24:46,074 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:46,074 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:46,075 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:46,097 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:46,097 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:46,099 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:46,123 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:46,125 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:46,126 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:46,152 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:46,152 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:46,152 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:46,152 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:46,153 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:46,156 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:46,157 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:46,157 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:46,165 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:46,165 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:46,166 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:46,166 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:46,174 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:46,174 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.7.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:46,174 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.7.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:46,178 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.input_layernorm.weight, converting to float16
2025-08-04 18:24:46,178 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:46,181 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:46,181 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:46,203 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:46,204 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:46,204 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:46,223 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:46,225 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:46,226 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:46,252 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:46,252 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:46,252 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:46,252 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:46,253 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:46,257 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:46,258 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:46,258 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:46,265 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:46,265 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:46,265 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:46,265 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:46,272 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:46,272 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.8.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:46,272 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.8.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:46,277 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.input_layernorm.weight, converting to float16
2025-08-04 18:24:46,277 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.down_proj.biases, converting to float16
2025-08-04 18:24:46,278 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.down_proj.scales, converting to float16
2025-08-04 18:24:46,278 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.down_proj.weight, converting to float16
2025-08-04 18:24:46,295 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.gate_proj.biases, converting to float16
2025-08-04 18:24:46,296 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.gate_proj.scales, converting to float16
2025-08-04 18:24:46,297 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.gate_proj.weight, converting to float16
2025-08-04 18:24:46,316 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.up_proj.biases, converting to float16
2025-08-04 18:24:46,317 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.mlp.up_proj.scales, converting to float16
2025-08-04 18:24:46,318 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.mlp.up_proj.weight, converting to float16
2025-08-04 18:24:46,339 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.post_attention_layernorm.weight, converting to float16
2025-08-04 18:24:46,339 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_norm.weight, converting to float16
2025-08-04 18:24:46,339 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_proj.biases, converting to float16
2025-08-04 18:24:46,339 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.k_proj.scales, converting to float16
2025-08-04 18:24:46,340 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.k_proj.weight, converting to float16
2025-08-04 18:24:46,344 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.o_proj.biases, converting to float16
2025-08-04 18:24:46,345 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.o_proj.scales, converting to float16
2025-08-04 18:24:46,345 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.o_proj.weight, converting to float16
2025-08-04 18:24:46,355 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_norm.weight, converting to float16
2025-08-04 18:24:46,355 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_proj.biases, converting to float16
2025-08-04 18:24:46,355 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.q_proj.scales, converting to float16
2025-08-04 18:24:46,356 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.q_proj.weight, converting to float16
2025-08-04 18:24:46,364 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.v_proj.biases, converting to float16
2025-08-04 18:24:46,364 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.layers.9.self_attn.v_proj.scales, converting to float16
2025-08-04 18:24:46,364 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.uint32 for model.layers.9.self_attn.v_proj.weight, converting to float16
2025-08-04 18:24:46,369 - [src.utils.mlx_pytorch_adapter] - WARNING - Unknown weight dtype torch.bfloat16 for model.norm.weight, converting to float16
2025-08-04 18:24:46,369 - [src.utils.mlx_pytorch_adapter] - INFO - Converted 704 weights
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.embed_tokens.weight: expected torch.Size([151936, 2048]), got torch.Size([151936, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.0.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.1.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,023 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.2.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.3.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.4.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.5.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.6.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,024 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.7.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.8.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.9.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.10.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.11.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.12.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,025 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.13.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.14.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.15.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.16.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.17.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.18.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,026 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.19.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.20.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.21.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.22.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.23.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.24.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,027 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.25.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.26.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.q_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.k_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.v_proj.weight: expected torch.Size([1024, 2048]), got torch.Size([1024, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.self_attn.o_proj.weight: expected torch.Size([2048, 2048]), got torch.Size([2048, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.gate_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.up_proj.weight: expected torch.Size([6144, 2048]), got torch.Size([6144, 512])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - WARNING - Shape mismatch for model.layers.27.mlp.down_proj.weight: expected torch.Size([2048, 6144]), got torch.Size([2048, 1536])
2025-08-04 18:25:05,028 - [src.utils.mlx_pytorch_adapter] - INFO - Loaded 113 weights, missing 198
2025-08-04 18:25:06,864 - [__main__] - INFO - Successfully loaded model with 28 assigned layers
2025-08-04 18:25:06,865 - [__main__] - INFO - Initializing KV cache...
2025-08-04 18:25:06,865 - [__main__] - ERROR - Failed to initialize KV cache: estimate_kv_cache_memory() got an unexpected keyword argument 'num_heads'
2025-08-04 18:25:06,865 - [__main__] - ERROR - Server initialization failed: estimate_kv_cache_memory() got an unexpected keyword argument 'num_heads'
2025-08-04 18:25:06,865 - [__main__] - ERROR - Server initialization failed
2025-08-04 18:25:06,865 - [__main__] - INFO - Shutting down server...
2025-08-04 18:25:06,865 - [src.coordination.file_based_coordinator] - INFO - Cleaned up coordination files
