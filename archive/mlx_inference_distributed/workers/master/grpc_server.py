"""
gRPC server implementation for distributed MLX inference.

This module provides the server-side implementation for model sharding
and distributed inference across heterogeneous devices.
"""

import grpc
from concurrent import futures
import mlx.core as mx
import numpy as np
import time
import logging
import asyncio
from typing import Dict, Optional, List, Tuple, Any
import threading
import psutil
import json

# Setup logger first
logger = logging.getLogger(__name__)

# Import generated protobuf modules (will be generated by generate_proto.sh)
try:
    import distributed_inference_pb2 as pb2
    import distributed_inference_pb2_grpc as pb2_grpc
except ImportError:
    logger.warning("Protocol buffer modules not found. Run generate_proto.sh first.")
    pb2 = None
    pb2_grpc = None

from model_abstraction import ModelFactory, ModelShard, BaseModelWrapper, ModelInfo
from device_capabilities import DeviceCapabilityDetector, DeviceProfile

logger = logging.getLogger(__name__)


class TensorSerializer:
    """Handles serialization/deserialization of MLX tensors for gRPC."""
    
    @staticmethod
    def tensor_to_proto(tensor: mx.array) -> pb2.Tensor:
        """Convert MLX array to protobuf Tensor."""
        # Ensure tensor is evaluated
        mx.eval(tensor)
        
        # Store original MLX dtype
        original_dtype = str(tensor.dtype)
        
        # Convert to numpy for serialization
        # Handle bfloat16 which numpy doesn't support directly
        if tensor.dtype == mx.bfloat16:
            # Convert to float32 for serialization
            tensor_f32 = tensor.astype(mx.float32)
            np_array = np.array(tensor_f32, dtype=np.float32)
            serialization_dtype = "float32"
        else:
            np_array = np.array(tensor)
            serialization_dtype = str(np_array.dtype)
        
        proto_tensor = pb2.Tensor(
            shape=list(np_array.shape),
            dtype=serialization_dtype,
            data=np_array.tobytes()
        )
        
        # Store original MLX dtype in metadata if conversion was needed
        if original_dtype != serialization_dtype:
            proto_tensor.metadata["original_mlx_dtype"] = original_dtype
        
        return proto_tensor
    
    @staticmethod
    def proto_to_tensor(proto_tensor: pb2.Tensor) -> mx.array:
        """Convert protobuf Tensor to MLX array."""
        # Reconstruct numpy array
        np_array = np.frombuffer(
            proto_tensor.data,
            dtype=np.dtype(proto_tensor.dtype)
        ).reshape(proto_tensor.shape)
        
        # Convert to MLX array
        mlx_tensor = mx.array(np_array)
        
        # Restore original dtype if it was converted
        if "original_mlx_dtype" in proto_tensor.metadata:
            original_dtype = proto_tensor.metadata["original_mlx_dtype"]
            if original_dtype == "mlx.core.bfloat16":
                mlx_tensor = mlx_tensor.astype(mx.bfloat16)
            elif original_dtype == "mlx.core.float16":
                mlx_tensor = mlx_tensor.astype(mx.float16)
            # Add other dtype conversions as needed
        
        return mlx_tensor


class DistributedInferenceServicer(pb2_grpc.DistributedInferenceServicer):
    """gRPC service implementation for distributed inference."""
    
    def __init__(self, device_id: str, port: int):
        self.device_id = device_id
        self.port = port
        self.model_wrapper: Optional[BaseModelWrapper] = None
        self.model_shard: Optional[ModelShard] = None
        self.device_profile: Optional[DeviceProfile] = None
        self.shard_info: Optional[pb2.ShardInfo] = None
        
        # Statistics
        self.start_time = time.time()
        self.requests_processed = 0
        self._stats_lock = threading.Lock()
        
        # Detect device capabilities
        self._detect_capabilities()
        
        logger.info(f"Initialized inference server on {device_id}:{port}")
    
    def _detect_capabilities(self):
        """Detect and store device capabilities."""
        detector = DeviceCapabilityDetector()
        self.device_profile = detector.detect_capabilities()
        logger.info(f"Detected capabilities: {self.device_profile}")
    
    def InitializeShard(self, request: pb2.InitializeShardRequest, 
                       context: grpc.ServicerContext) -> pb2.InitializeShardResponse:
        """Initialize model shard on this device."""
        start_time = time.time()
        
        try:
            shard_info = request.shard_info
            logger.info(f"Initializing shard: layers {shard_info.start_layer}-{shard_info.end_layer-1} "
                       f"for {request.model_name} on device {shard_info.device_id}")
            
            # Create model wrapper
            self.model_wrapper = ModelFactory.create_wrapper(request.model_name)
            
            # Load the full model first (we'll optimize to load only needed parts later)
            self.model_wrapper.load_model()
            logger.info(f"Model loaded: {self.model_wrapper.model_info.num_layers} layers total")
            
            # Extract cluster configuration from request to determine total devices
            # For now, infer from the layer assignments in the request
            total_layers = self.model_wrapper.model_info.num_layers
            assigned_layers = shard_info.end_layer - shard_info.start_layer
            
            # Validate layer assignment
            if shard_info.start_layer < 0 or shard_info.end_layer > total_layers:
                raise ValueError(f"Invalid layer range: {shard_info.start_layer}-{shard_info.end_layer}, "
                               f"model has {total_layers} layers")
            
            if shard_info.start_layer >= shard_info.end_layer:
                raise ValueError(f"Invalid layer range: start_layer {shard_info.start_layer} >= "
                               f"end_layer {shard_info.end_layer}")
            
            # Create a custom shard for exactly the assigned layers
            self.model_shard = self._create_specific_shard(
                shard_info.start_layer, 
                shard_info.end_layer,
                shard_info.device_id
            )
            
            self.shard_info = shard_info
            
            load_time_ms = int((time.time() - start_time) * 1000)
            
            # Calculate actual shard size based on assigned layers
            shard_size_bytes = self._estimate_shard_size(shard_info.start_layer, shard_info.end_layer)
            
            logger.info(f"Successfully initialized shard on {self.device_id}: "
                       f"layers {shard_info.start_layer}-{shard_info.end_layer-1} "
                       f"({assigned_layers} layers, {shard_size_bytes / (1024**3):.2f} GB)")
            
            return pb2.InitializeShardResponse(
                success=True,
                message=f"Successfully initialized shard with {assigned_layers} layers on {self.device_id}",
                load_time_ms=load_time_ms,
                model_size_bytes=shard_size_bytes
            )
            
        except Exception as e:
            logger.error(f"Failed to initialize shard: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return pb2.InitializeShardResponse(
                success=False,
                message=str(e),
                load_time_ms=0,
                model_size_bytes=0
            )
    
    def Forward(self, request: pb2.ForwardRequest, 
               context: grpc.ServicerContext) -> pb2.ForwardResponse:
        """Process forward pass through this device's model shard."""
        start_time = time.time()
        
        try:
            if not self.model_shard:
                raise ValueError("Model shard not initialized")
            
            # Deserialize input tensor
            input_tensor = TensorSerializer.proto_to_tensor(request.input_tensor)
            
            # Deserialize cache if provided
            cache = None
            if request.cache:
                cache = []
                for key in sorted(request.cache.keys()):
                    cache_tensor = TensorSerializer.proto_to_tensor(request.cache[key])
                    cache.append(cache_tensor)
            
            # Forward pass through shard
            output_tensor = self.model_shard(input_tensor, cache=cache)
            
            # Ensure computation is complete
            mx.eval(output_tensor)
            
            # Update statistics
            with self._stats_lock:
                self.requests_processed += 1
            
            inference_time_ms = int((time.time() - start_time) * 1000)
            
            # Prepare response
            response = pb2.ForwardResponse(
                request_id=request.request_id,
                output_tensor=TensorSerializer.tensor_to_proto(output_tensor),
                inference_time_ms=inference_time_ms
            )
            
            # Add updated cache if requested
            if request.return_cache and cache:
                for i, cache_tensor in enumerate(cache):
                    response.cache[str(i)] = TensorSerializer.tensor_to_proto(cache_tensor)
            
            return response
            
        except Exception as e:
            logger.error(f"Forward pass failed: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return pb2.ForwardResponse()
    
    def HealthCheck(self, request: pb2.HealthCheckRequest,
                   context: grpc.ServicerContext) -> pb2.HealthCheckResponse:
        """Health check endpoint."""
        try:
            # Get system stats
            gpu_util = 0.0  # MLX doesn't provide direct GPU utilization
            memory_util = psutil.virtual_memory().percent
            
            uptime = int(time.time() - self.start_time)
            
            response = pb2.HealthCheckResponse(
                healthy=True,
                status="healthy",
                uptime_seconds=uptime,
                gpu_utilization=gpu_util,
                memory_utilization=memory_util,
                requests_processed=self.requests_processed
            )
            
            if request.include_stats:
                response.stats["device_id"] = self.device_id
                response.stats["model_loaded"] = str(self.model_shard is not None)
                if self.shard_info:
                    response.stats["shard_layers"] = f"{self.shard_info.start_layer}-{self.shard_info.end_layer}"
            
            return response
            
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return pb2.HealthCheckResponse(
                healthy=False,
                status=f"error: {str(e)}",
                uptime_seconds=0
            )
    
    def GetCapabilities(self, request: pb2.GetCapabilitiesRequest,
                       context: grpc.ServicerContext) -> pb2.GetCapabilitiesResponse:
        """Return device capabilities."""
        if not self.device_profile:
            self._detect_capabilities()
        
        capabilities = pb2.DeviceCapabilities(
            device_id=self.device_profile.device_id,
            device_model=self.device_profile.model,
            memory_bytes=int(self.device_profile.memory_gb * 1024**3),
            gpu_cores=self.device_profile.gpu_cores,
            cpu_cores=self.device_profile.cpu_cores
        )
        
        # Add extra info
        capabilities.extra["cpu_performance_cores"] = str(self.device_profile.cpu_performance_cores)
        capabilities.extra["cpu_efficiency_cores"] = str(self.device_profile.cpu_efficiency_cores)
        capabilities.extra["neural_engine_cores"] = str(self.device_profile.neural_engine_cores)
        capabilities.extra["max_recommended_model_gb"] = str(self.device_profile.max_recommended_model_size_gb)
        
        response = pb2.GetCapabilitiesResponse(
            capabilities=capabilities,
            supported_models=ModelFactory.get_supported_models(),
            supported_dtypes=["float32", "float16", "bfloat16", "int8", "int4"]
        )
        
        return response
    
    def Shutdown(self, request: pb2.ShutdownRequest,
                context: grpc.ServicerContext) -> pb2.ShutdownResponse:
        """Shutdown the server."""
        try:
            logger.info(f"Shutdown requested (force={request.force}, "
                       f"grace_period={request.grace_period_seconds}s)")
            
            # Clean up model
            if self.model_wrapper:
                del self.model_wrapper
                self.model_wrapper = None
                self.model_shard = None
            
            # Schedule shutdown
            if request.grace_period_seconds > 0 and not request.force:
                threading.Timer(request.grace_period_seconds, self._shutdown).start()
                message = f"Shutdown scheduled in {request.grace_period_seconds} seconds"
            else:
                self._shutdown()
                message = "Immediate shutdown initiated"
            
            return pb2.ShutdownResponse(
                success=True,
                message=message
            )
            
        except Exception as e:
            logger.error(f"Shutdown failed: {e}")
            return pb2.ShutdownResponse(
                success=False,
                message=str(e)
            )
    
    def _create_specific_shard(self, start_layer: int, end_layer: int, device_id: str) -> ModelShard:
        """Create a model shard for specific layer range.
        
        Args:
            start_layer: Starting layer index (inclusive)
            end_layer: Ending layer index (exclusive)
            device_id: ID of the device this shard will run on
            
        Returns:
            ModelShard configured for the specified layers
        """
        if not self.model_wrapper or not self.model_wrapper.model:
            raise ValueError("Model not loaded")
        
        # Access the actual model layers
        if hasattr(self.model_wrapper.model, 'model'):
            base_model = self.model_wrapper.model.model
        else:
            base_model = self.model_wrapper.model
        
        layers = base_model.layers
        total_layers = len(layers)
        
        # Validate layer range
        if start_layer < 0 or end_layer > total_layers or start_layer >= end_layer:
            raise ValueError(f"Invalid layer range {start_layer}-{end_layer} for model with {total_layers} layers")
        
        # Extract the specific layers for this shard
        shard_layers = layers[start_layer:end_layer]
        shard_indices = list(range(start_layer, end_layer))
        
        # Determine special components based on layer position
        is_first_shard = (start_layer == 0)
        is_last_shard = (end_layer == total_layers)
        
        # Get embedding tokens if this is the first shard
        embed_tokens = base_model.embed_tokens if is_first_shard else None
        
        # Get norm if this is the last shard
        norm = base_model.norm if is_last_shard else None
        
        # Handle lm_head and tied embeddings for last shard
        lm_head = None
        use_tied_embeddings = False
        
        if is_last_shard:
            # Check for tied embeddings in multiple possible locations
            if hasattr(self.model_wrapper.model, 'args'):
                use_tied_embeddings = getattr(self.model_wrapper.model.args, 'tie_word_embeddings', False)
            elif hasattr(self.model_wrapper.config, 'tie_word_embeddings'):
                use_tied_embeddings = getattr(self.model_wrapper.config, 'tie_word_embeddings', False)
            
            # Try to get lm_head
            lm_head = getattr(self.model_wrapper.model, 'lm_head', None)
            if lm_head is not None:
                pass  # lm_head is already set
            elif use_tied_embeddings:
                # For tied embeddings, we need embedding tokens for output projection
                embed_tokens = base_model.embed_tokens
                logger.info(f"Using tied embeddings for output projection on {device_id}")
            else:
                logger.warning(f"No lm_head found and tie_word_embeddings not set for {device_id}")
        
        # Create the model shard
        shard = ModelShard(
            layers=shard_layers,
            layer_indices=shard_indices,
            embed_tokens=embed_tokens,
            norm=norm,
            lm_head=lm_head,
            use_tied_embeddings=use_tied_embeddings
        )
        
        logger.info(f"Created specific shard for {device_id}: "
                   f"layers {start_layer}-{end_layer-1}, "
                   f"embed={embed_tokens is not None}, "
                   f"norm={norm is not None}, "
                   f"lm_head={lm_head is not None}, "
                   f"tied_embeddings={use_tied_embeddings}")
        
        return shard
    
    def _estimate_shard_size(self, start_layer: int, end_layer: int) -> int:
        """Estimate memory size of a specific shard in bytes.
        
        Args:
            start_layer: Starting layer index (inclusive)
            end_layer: Ending layer index (exclusive)
            
        Returns:
            Estimated memory size in bytes
        """
        if not self.model_wrapper or not self.model_wrapper.model_info:
            return 0
        
        model_info = self.model_wrapper.model_info
        num_shard_layers = end_layer - start_layer
        total_layers = model_info.num_layers
        
        # Base layer memory calculation
        layer_memory_gb = self._estimate_layer_memory_gb(model_info, num_shard_layers)
        
        # Add embedding memory if this shard includes layer 0
        if start_layer == 0:
            layer_memory_gb += self._estimate_embedding_memory_gb(model_info)
        
        # Add head memory if this shard includes the last layer
        if end_layer == total_layers:
            layer_memory_gb += self._estimate_head_memory_gb(model_info)
        
        return int(layer_memory_gb * 1024**3)
    
    def _estimate_layer_memory_gb(self, model_info: ModelInfo, num_layers: int) -> float:
        """Estimate memory for transformer layers in GB."""
        # Estimate based on model architecture
        total_model_gb = model_info.estimate_size_gb()
        
        # Assume 85% of model size is in transformer layers
        layer_fraction = 0.85
        layer_memory_gb = (total_model_gb * layer_fraction * num_layers) / model_info.num_layers
        
        return layer_memory_gb
    
    def _estimate_embedding_memory_gb(self, model_info: ModelInfo) -> float:
        """Estimate memory for embedding layer in GB."""
        bytes_per_param = 1 if 'int8' in str(model_info.quantization) else 2
        embedding_gb = (model_info.vocab_size * model_info.hidden_size * bytes_per_param) / (1024**3)
        return embedding_gb
    
    def _estimate_head_memory_gb(self, model_info: ModelInfo) -> float:
        """Estimate memory for language model head in GB."""
        bytes_per_param = 1 if 'int8' in str(model_info.quantization) else 2
        head_gb = (model_info.hidden_size * model_info.vocab_size * bytes_per_param) / (1024**3)
        return head_gb

    def _shutdown(self):
        """Perform actual shutdown."""
        logger.info("Shutting down gRPC server...")
        # Server shutdown will be handled by the serve() function


class DistributedInferenceServer:
    """Main server class for distributed inference."""
    
    def __init__(self, device_id: str, port: int, max_workers: int = 10):
        self.device_id = device_id
        self.port = port
        self.max_workers = max_workers
        self.server = None
        self.servicer = None
    
    def start(self):
        """Start the gRPC server."""
        # Create servicer
        self.servicer = DistributedInferenceServicer(self.device_id, self.port)
        
        # Create server
        self.server = grpc.server(
            futures.ThreadPoolExecutor(max_workers=self.max_workers),
            options=[
                ('grpc.max_send_message_length', 1024 * 1024 * 1024),  # 1GB
                ('grpc.max_receive_message_length', 1024 * 1024 * 1024),  # 1GB
            ]
        )
        
        # Add servicer
        pb2_grpc.add_DistributedInferenceServicer_to_server(self.servicer, self.server)
        
        # Bind to port
        address = f'[::]:{self.port}'
        self.server.add_insecure_port(address)
        
        # Start server
        self.server.start()
        logger.info(f"gRPC server started on {address}")
        
        return self
    
    def wait_for_termination(self):
        """Block until server terminates."""
        if self.server:
            self.server.wait_for_termination()
    
    def stop(self, grace_period: float = 5.0):
        """Stop the server gracefully."""
        if self.server:
            logger.info(f"Stopping server with {grace_period}s grace period...")
            self.server.stop(grace_period)


def serve(device_id: str = None, port: int = 50051):
    """Main entry point for running the inference server."""
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Auto-detect device ID if not provided
    if not device_id:
        import socket
        device_id = socket.gethostname().split('.')[0]
    
    logger.info(f"Starting distributed inference server: {device_id}:{port}")
    
    # Create and start server
    server = DistributedInferenceServer(device_id, port)
    server.start()
    
    try:
        server.wait_for_termination()
    except KeyboardInterrupt:
        logger.info("Keyboard interrupt received")
        server.stop()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='MLX Distributed Inference Server')
    parser.add_argument('--device-id', type=str, help='Device ID (auto-detected if not provided)')
    parser.add_argument('--port', type=int, default=50051, help='Server port (default: 50051)')
    
    args = parser.parse_args()
    
    serve(device_id=args.device_id, port=args.port)