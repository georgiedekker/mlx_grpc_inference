experiment:
  name: training_alpaca
  tags:
  - mlx
  - alpaca
  - lora
model:
  name: mlx-community/Qwen2.5-1.5B-4bit
  quantization: 4bit
training:
  batch_size: 4
  learning_rate: 5.0e-05
  num_epochs: 3
  gradient_accumulation_steps: 4
  warmup_ratio: 0.1
  save_steps: 100
  eval_steps: 50
data:
  format: alpaca
  train_path: data/train.alpaca.json
  validation_split: 0.1
  max_seq_length: 2048
lora:
  enabled: true
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules:
  - q_proj
  - v_proj
distributed:
  enabled: false
  strategy: data_parallel
logging:
  level: INFO
  wandb:
    enabled: false
    project: training-mlx
  tensorboard:
    enabled: true
    log_dir: ./logs
