dataset_format: alpaca
dataset_path: alpaca_example.json
experiment_name: sample_lora_training
lora_params:
  lora_alpha: 16
  lora_dropout: 0.05
  lora_r: 8
  lora_target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  use_lora: true
  use_qlora: false
model_name: mlx-community/Qwen2.5-1.5B-4bit
output_dir: ./lora_model_output
training_params:
  batch_size: 4
  epochs: 3
  learning_rate: 5.0e-05
  save_steps: 500
  warmup_steps: 100
