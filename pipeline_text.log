2025-08-06 23:47:57,380 - INFO - Loading model: mlx-community/Qwen3-1.7B-8bit
2025-08-06 23:47:57,366 - INFO - Loading model: mlx-community/Qwen3-1.7B-8bit
2025-08-06 23:47:57,418 - INFO - Distributed initialized: rank=0, world_size=2
2025-08-06 23:47:57,418 - INFO - Rank 0: Downloading model metadata...
2025-08-06 23:47:57,441 - INFO - Distributed initialized: rank=1, world_size=2
2025-08-06 23:47:57,441 - INFO - Rank 1: Downloading model metadata...
2025-08-06 23:47:57,419 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-08-06 23:47:57,442 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-08-06 23:47:57,605 - DEBUG - https://huggingface.co:443 "GET /api/models/mlx-community/Qwen3-1.7B-8bit/revision/main HTTP/1.1" 200 6006
2025-08-06 23:47:57,634 - DEBUG - https://huggingface.co:443 "GET /api/models/mlx-community/Qwen3-1.7B-8bit/revision/main HTTP/1.1" 200 6006
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 109297.82it/s]
2025-08-06 23:47:57,622 - INFO - Rank 0: Lazy loading model to determine sharding...
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 86037.01it/s]
2025-08-06 23:47:57,657 - INFO - Rank 1: Lazy loading model to determine sharding...
2025-08-06 23:47:57,640 - WARNING - Model mlx-community/Qwen3-1.7B-8bit doesn't have native pipeline() support
2025-08-06 23:47:57,640 - INFO - Adding custom pipeline() implementation...
2025-08-06 23:47:57,640 - INFO - Added pipeline() method to Qwen3Model
2025-08-06 23:47:57,640 - INFO - âœ… Custom pipeline() method added successfully!
2025-08-06 23:47:57,640 - INFO - Rank 0: Applying pipeline sharding...
2025-08-06 23:47:57,640 - INFO - Applying pipeline parallelism: rank 0/2
2025-08-06 23:47:57,640 - INFO - Found 28 transformer layers
2025-08-06 23:47:57,640 - INFO - Rank 0: Assigned layers 0-13
2025-08-06 23:47:57,640 - INFO - Rank 0: Owns layers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
2025-08-06 23:47:57,640 - INFO - Rank 0: has_embeddings=True, has_output=False
2025-08-06 23:47:57,640 - INFO - Rank 0: Pipeline setup complete
2025-08-06 23:47:57,641 - INFO - Rank 0: Downloading 1 weight files for this shard...
2025-08-06 23:47:57,681 - WARNING - Model mlx-community/Qwen3-1.7B-8bit doesn't have native pipeline() support
2025-08-06 23:47:57,681 - INFO - Adding custom pipeline() implementation...
2025-08-06 23:47:57,681 - INFO - Added pipeline() method to Qwen3Model
2025-08-06 23:47:57,681 - INFO - âœ… Custom pipeline() method added successfully!
2025-08-06 23:47:57,681 - INFO - Rank 1: Applying pipeline sharding...
2025-08-06 23:47:57,681 - INFO - Applying pipeline parallelism: rank 1/2
2025-08-06 23:47:57,681 - INFO - Found 28 transformer layers
2025-08-06 23:47:57,681 - INFO - Rank 1: Assigned layers 14-27
2025-08-06 23:47:57,681 - INFO - Rank 1: Owns layers [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
2025-08-06 23:47:57,682 - INFO - Rank 1: has_embeddings=False, has_output=True
2025-08-06 23:47:57,682 - INFO - Rank 1: Pipeline setup complete
2025-08-06 23:47:57,683 - INFO - Rank 1: Downloading 1 weight files for this shard...
2025-08-06 23:47:57,767 - DEBUG - https://huggingface.co:443 "GET /api/models/mlx-community/Qwen3-1.7B-8bit/revision/main HTTP/1.1" 200 6006
Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 9776.93it/s]
2025-08-06 23:47:57,814 - DEBUG - https://huggingface.co:443 "GET /api/models/mlx-community/Qwen3-1.7B-8bit/revision/main HTTP/1.1" 200 6006
Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2197.12it/s]
2025-08-06 23:47:58,058 - INFO - Rank 0: Loading model weights for this shard...
2025-08-06 23:47:58,093 - INFO - Rank 1: Loading model weights for this shard...
2025-08-06 23:47:58,148 - INFO - Rank 0: GPU memory after loading = 1.70 GB
2025-08-06 23:47:58,171 - INFO - Rank 1: GPU memory after loading = 1.70 GB
2025-08-06 23:47:58,213 - INFO - Rank 0: Ready for distributed inference!
2025-08-06 23:47:58,214 - INFO - Starting API server on rank 0
2025-08-06 23:47:58,241 - INFO - Rank 1: Ready for distributed inference!
2025-08-06 23:47:58,241 - INFO - Worker rank 1 ready for distributed processing
[32mINFO[0m:     Started server process [[36m8518[0m]
[32mINFO[0m:     Waiting for application startup.
[32mINFO[0m:     Application startup complete.
[32mINFO[0m:     Uvicorn running on [1mhttp://0.0.0.0:8100[0m (Press CTRL+C to quit)
2025-08-06 23:48:15,061 - DEBUG - Token 1: text='<think>'
2025-08-06 23:48:15,062 - DEBUG - Token 1: decoded='<think>'
2025-08-06 23:48:15,083 - DEBUG - Token 2: text='
'
2025-08-06 23:48:15,083 - DEBUG - Token 2: decoded='
'
2025-08-06 23:48:15,103 - DEBUG - Token 3: text='Okay'
2025-08-06 23:48:15,103 - DEBUG - Token 3: decoded='Okay'
2025-08-06 23:48:15,122 - DEBUG - Token 4: text=','
2025-08-06 23:48:15,123 - DEBUG - Token 4: decoded=','
2025-08-06 23:48:15,142 - DEBUG - Token 5: text=' the'
2025-08-06 23:48:15,142 - DEBUG - Token 5: decoded=' the'
2025-08-06 23:48:15,259 - INFO - âœ… Generated 10 tokens using 2 GPUs
2025-08-06 23:48:15,259 - INFO - Prompt: 15 tokens, 164.1 tok/s
2025-08-06 23:48:15,259 - INFO - Generation: 10 tokens, 55.1 tok/s
2025-08-06 23:48:15,259 - INFO - Peak memory: 1.71 GB
2025-08-06 23:48:15,259 - INFO - Generated text: '<think>
Okay, the user is asking "WhatWhat' (length: 42)
[32mINFO[0m:     127.0.0.1:58235 - "[1mPOST /v1/chat/completions HTTP/1.1[0m" [32m200 OK[0m
2025-08-06 23:48:24,607 - DEBUG - Token 1: text='<think>'
2025-08-06 23:48:24,607 - DEBUG - Token 1: decoded='<think>'
2025-08-06 23:48:24,630 - DEBUG - Token 2: text='
'
2025-08-06 23:48:24,630 - DEBUG - Token 2: decoded='
'
2025-08-06 23:48:24,650 - DEBUG - Token 3: text='Okay'
2025-08-06 23:48:24,650 - DEBUG - Token 3: decoded='Okay'
2025-08-06 23:48:24,671 - DEBUG - Token 4: text=','
2025-08-06 23:48:24,671 - DEBUG - Token 4: decoded=','
2025-08-06 23:48:24,692 - DEBUG - Token 5: text=' the'
2025-08-06 23:48:24,692 - DEBUG - Token 5: decoded=' the'
2025-08-06 23:48:25,215 - INFO - âœ… Generated 30 tokens using 2 GPUs
2025-08-06 23:48:25,215 - INFO - Prompt: 15 tokens, 152.5 tok/s
2025-08-06 23:48:25,215 - INFO - Generation: 30 tokens, 50.6 tok/s
2025-08-06 23:48:25,215 - INFO - Peak memory: 1.71 GB
2025-08-06 23:48:25,215 - INFO - Generated text: '<think>
Okay, the user is asking "What is 2+2?" That's a basic arithmetic question. Let me think about how to approach approach' (length: 127)
[32mINFO[0m:     127.0.0.1:58267 - "[1mPOST /v1/chat/completions HTTP/1.1[0m" [32m200 OK[0m
2025-08-06 23:48:47,962 - DEBUG - Token 1: text='<think>'
2025-08-06 23:48:47,962 - DEBUG - Token 1: decoded='<think>'
2025-08-06 23:48:47,986 - DEBUG - Token 2: text='
'
2025-08-06 23:48:47,986 - DEBUG - Token 2: decoded='
'
2025-08-06 23:48:48,005 - DEBUG - Token 3: text='Okay'
2025-08-06 23:48:48,005 - DEBUG - Token 3: decoded='Okay'
2025-08-06 23:48:48,025 - DEBUG - Token 4: text=','
2025-08-06 23:48:48,025 - DEBUG - Token 4: decoded=','
2025-08-06 23:48:48,044 - DEBUG - Token 5: text=' I'
2025-08-06 23:48:48,044 - DEBUG - Token 5: decoded=' I'
2025-08-06 23:48:48,989 - INFO - âœ… Generated 50 tokens using 2 GPUs
2025-08-06 23:48:48,989 - INFO - Prompt: 15 tokens, 158.3 tok/s
2025-08-06 23:48:48,989 - INFO - Generation: 50 tokens, 49.4 tok/s
2025-08-06 23:48:48,989 - INFO - Peak memory: 1.72 GB
2025-08-06 23:48:48,989 - INFO - Generated text: '<think>
Okay, I need to write a Python function to calculate the factorial of a number. Let me think about how to approach this. 

First, what is a factorial? The factorial of a non-negative integer n, denoted as n!,!,' (length: 218)
[32mINFO[0m:     127.0.0.1:58348 - "[1mPOST /v1/chat/completions HTTP/1.1[0m" [32m200 OK[0m
[32mINFO[0m:     Shutting down
[32mINFO[0m:     Waiting for application shutdown.
[32mINFO[0m:     Application shutdown complete.
[32mINFO[0m:     Finished server process [[36m8518[0m]
/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/resource_tracker.py:301: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown: {'/mp-3iab232p'}
  warnings.warn(
